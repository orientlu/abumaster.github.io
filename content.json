[{"title":"C++类构造函数","date":"2017-04-02T06:45:54.000Z","path":"2017/04/02/C-类构造函数/","text":"###C++中构造函数和析构函数应该注意的问题 构造方法用来初始化类的对象，与父类的其它成员不同，它不能被子类继承（子类可以继承父类所有的成员变量和成员方法，但不继承父类的构造方法）。因此，在创建子类对象时，为了初始化从父类继承来的数据成员，系统需要调用其父类的构造方法。C++11新标准中，派生类可以重用其直接基类定义的构造函数，类不能继承默认、拷贝、移动构造函数，如果派生类没有指定，则编译器会自动合成。 构造原则如下： 如果子类没有定义构造方法，则调用父类的无参数的构造方法。 如果子类定义了构造方法，不论是无参数还是带参数，在创建子类的对象的时候,首先执行父类无参数的构造方法，然后执行自己的构造方法。 在创建子类对象时候，如果子类的构造函数没有显示调用父类的构造函数，则会调用父类的默认无参构造函数。 在创建子类对象时候，如果子类的构造函数没有显示调用父类的构造函数且父类自己提供了无参构造函数，则会调用父类自己的无参构造函数。 在创建子类对象时候，如果子类的构造函数没有显示调用父类的构造函数且父类只定义了自己的有参构造函数，则会出错（如果父类只有有参数的构造方法，则子类必须显示调用此带参构造方法）。 如果子类调用父类带参数的构造方法，需要用初始化父类成员对象的方式 析构函数基类的析构函数声明为虚函数，这样销毁对象时子类会调用子类的析构函数，防止内存泄漏。如果没有定义为虚析构函数，销毁一个子类或者父类对象时，都会调用父类析构函数。","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]},{"title":"数据结构-红黑二叉树","date":"2017-04-01T04:25:53.000Z","path":"2017/04/01/数据结构-红黑二叉树/","text":"红黑树（Red Black Tree）是一种自平衡二叉查找树，是在计算机科学中用到的一种数据结构，典型的用途是实现关联数组。它是在1972年由Rudolf Bayer发明的，当时被称为平衡二叉B树（symmetric binary B-trees）。后来，在1978年被 Leo J. Guibas 和 Robert Sedgewick 修改为如今的“红黑树”。红黑树和AVL树（平衡二叉树）类似，都是在进行插入和删除操作时通过特定操作保持二叉查找树的平衡，从而获得较高的查找性能，而统计性能要优于AVL树，广泛应用到各种程序库中。它虽然是复杂的，但它的最坏情况运行时间也是非常良好的，并且在实践中是高效的： 它可以在O(log n)时间内做查找，插入和删除，这里的n是树中元素的数目。性质红黑树是每个节点都带有黑色或者红色的二叉查找树。具有二叉树的性质，并且具有以下几个性质： 根节点是黑色 叶子节点（空节点）是黑色的 每个红色节点的两个子节点都是黑色的，叶子到根的路径上不能有连续的红色节点 从任一节点开始到其每个叶子节点的所有路径包含相同数目的黑色节点基本操作左旋、右旋、重新着色三个操作。右旋操作类似，左旋就是将旋转的节点变为左子树，提取右节点上来，右旋是将右旋节点变为右子树，提取左节点上来。 插入","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]},{"title":"动态规划-背包问题","date":"2017-03-31T00:41:50.000Z","path":"2017/03/31/动态规划-背包问题/","text":"题目描述有 a，b，c 三个物体，重量记为 W 5，4，3价值记为 V 20 10 12有一个背包容量 C = 10 ，问：可以装的最大价值为多少？ 解决动态规划问题的主要方法是找到状态转移方程，动态规划全局最优包含了局部最优解。分析上述问题：背包容量10，首先，第一个物品有装入和不装入两种情况，转入的话状态变为：容量5，物品重量4,3物品价值10,12；不装入则变为：容量10，物品质量4,3，价值10,12。因此可以定义：dp[i][j]表示前i个物品装到剩余容量为j的背包中的价值 dp[3][10]即为所求的结果，有了状态，这个状态是如何转移的呢？由上面的分析，可知，第i个物品有装入和不装入两种情况，因此状态转移方程可以表示如下：dp[i][j] = Max(dp[i-1][j], dp[i-1][j-w[i]]+v[i])。容易写出代码：123456789for(int i=0; i&lt;n; i++)&#123; for(int j=0; j&lt;=C; j++) &#123; dp[i][j] = (i==0?0:dp[i-1][j]); if(i&gt;0 &amp;&amp; j&gt;=W[i]) dp[i][j] = Max(dp[i-1][j], dp[i-1][j-w[i]]+v[i]); &#125;&#125; 关于优化空间复杂度上述存储状态方程为二维数组，可以压缩为一维数组，dp[i][j]变为dp[j]避免了重复的计算。123456789memeset(dp, 0, sizeof(dp));for(int i=0; i&lt;n; i++)&#123; for(int j=C; j&gt;=0; j++) &#123; if(i&gt;0 &amp;&amp; j&gt;=W[i]) dp[j] = Max(dp[j], dp[j-W[i]]+V[i]); &#125;&#125; [网易2017实习笔试题-双核处理]题目描述：一种双核CPU的两个核能够同时的处理任务，现在有n个已知数据量的任务需要交给CPU处理，假设已知CPU的每个核1秒可以处理1kb，每个核同时只能处理一项任务。n个任务可以按照任意顺序放入CPU进行处理，现在需要设计一个方案让CPU处理完这批任务所需的时间最少，求这个最小的时间。输入描述： 输入包括两行：第一行为整数n(1 ≤ n ≤ 50)第二行为n个整数lengthi，表示每个任务的长度为length[i]kb，每个数均为1024的倍数。 输出描述： 输出一个整数，表示最少需要处理的时间 输入输出例子： 53072 3072 7168 3072 10249216 解题思路:双核可以同时运行，故可以把任务分成两组，交由两个核顺序执行，最短执行时间取决于最后一个执行完成的时间，因此，两个数组长度相差越小，执行的时间也是越短的，换句话说，使其中一个数组无限接近输入数据总长度的一半sum/2即可。执行的时间为sum-sum/2。可以变为简单的背包问题：背包容量sum/2，物体重量为输入数据的长度，尽可能装满背包。状态转移方程可以记为：dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i]]+w[i]), dp[i][j]表示前i个物品在体积为j时可以填充的重量。 同样可以压缩数组变为一维，如上。代码：123456789101112131415161718192021#include &lt;iostream&gt; using namespace std; int dp[210000]; int n,arr[51]; int main() &#123; int n; scanf(\"%d\",&amp;n); int sum = 0; for(int i = 0 ; i &lt; n ; i ++)&#123; scanf(\"%d\",&amp;arr[i]); arr[i] /= 1024; sum += arr[i]; &#125; memset(dp, 0, sizeof(dp)); for(int i = 0 ; i &lt; n ; i ++) for(int j = sum/2 ; j &gt;= arr[i] ; --j) dp[j] = max(dp[j],dp[j-arr[i]]+arr[i]); printf(\"%d\\n\",(sum-dp[sum/2])*1024); return 0; &#125;","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]},{"title":"深度解析网络用于图像语义分割","date":"2017-03-30T02:20:52.000Z","path":"2017/03/30/深度解析网络用于图像语义分割/","text":"2015 年 ICCV 论文：Semantic Image Segmentation via Deep Parsing Network [1]，针对图像语义分割将丰富信息（上下文关系）并入马尔科夫随机场（MRF），取代用迭代法去优化 MRFs 提出了一种卷积网络，被称为深度解析网络（DPN），可以通过一次前向传递决定端对端的计算。 $$E(y)=\\sum_{\\forall{i}\\in \\upsilon}\\Phi({y_i^u})+\\sum_{\\forall{i,j}\\in\\varepsilon}\\Psi(y_i^u,y_j^v)$$主要贡献：使用DPN交叉训练VGG16网络，通过一次迭代近似MF，减少计算量并且保证性能。MRF对于一副图片，看成一个无向图。边代表像素之间的联系，顶点是一个二值隐变量可以看成像素i是否分到标签u。如公式所示：能量函数可以写为：\\y ,\\upsilon ,\\varepsilon\\分别代表了潜变量、顶点和边。上述能量函数分为一元项和二元项，很明显，一元项是一个预测值，表示预测像素是某一个标签，二元项则是代表一组平滑约束。$$\\Phi{(y_i^u)}=-\\ln{p\\left(y_i^u=1|I\\right)}$$像素i用标签u表示的可能性。对于二元项是距离和共存性的乘积决定的。不可能共存，则值很大。如果两个像素临近并且相似，那么将会被鼓励分配相同的标签。这种衡量方法有两个主要的缺点：1.第一项是从训练数据中获得两个标签同时发生的频率来衡量，忽略了两个标签的空间上下文信息，比如人可以出现在桌子旁边，但是不太可能在桌子下面或者上面。空间上下文是一个混合模式，不同物体的形态可能出现在不同的图片中。2.只在像素间定义了成对的关系，没有考虑到高阶的相互作用。不理解，先放着。","tags":[{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"},{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"}]},{"title":"公式专辑","date":"2017-03-29T07:53:21.000Z","path":"2017/03/29/公式专辑/","text":"公式的使用，论文中经常用到公式，在本地编写文章时常有 word 自带的基本上可以解决问题，当用 Markdown 书写时，又不想贴图，只好用在线的公式编辑器，一般有两种方法，一是在线生成公式，并引出外链，直接嵌入到文章中；另外一种用Mathjax引擎，引入一个脚本，在文章中编辑。 ###1.MathJax 引擎参考stackexchange，很简单引入一个脚本：1&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt; 然后编写公式一个栗子：1$$\\sum_&#123;i=0&#125;^n i^2 = \\frac&#123;(n^2+n)(2n+1)&#125;&#123;6&#125;$$ 就生成了：$$\\sum_{i=0}^n i^2 = \\frac{(n^2+n)(2n+1)}{6}$$缺点：不能在本地浏览器上预览 ###2.在线 LaTeX 公式编辑器在线LaTeX公式编辑器，使用LaTeX公式。在其中编写好公式后，直接生成了一段html代码，直接复制到 Markdown 文本中。如代码:1&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=$$f(x)=\\sum_&#123;i=1&#125;^n&amp;space;a_i$$&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?$$f(x)=\\sum_&#123;i=1&#125;^n&amp;space;a_i$$&quot; title=&quot;$$f(x)=\\sum_&#123;i=1&#125;^n a_i$$&quot; /&gt;&lt;/a&gt; 生成：优点：可以本地预览；缺点：只是引用的图片，右键不可操作，公式大的话可能加载慢，图片不清晰。 ###3.基本语法 无论使用哪种方式，公式的基本语法是相同的。常用的总结如下。 公式样式行内公式\\\\(公式\\\\)，行间公式$$公式$$**空格的表示：`\\quad’表示一个quad空格 字符\\为转义符，特殊字符前要加。 上下标用^表示上标，用_表示下标 字母上下标记用\\overline{}表示上划线，用\\underline{}表示下划线；用\\hat{}表示字母上面有一个小尖角，而\\widehat{}表示有一个大尖角;用\\bar{} \\acute{} \\check{} \\grave{}分别表示四个声调：一声平，二声扬，三声拐弯，四声降；\\tilde{}波浪线, \\vec{}向量,\\dot{}点。 希腊字符\\alpha, \\beta, …, \\omega: α,β,…ω；\\Gamma, \\Delta, …, \\Omega: Γ,Δ,…,Ω。 数学函数例如sin x要表示成\\sin x；log x要表示成\\log x；lim x表示成\\lim_{x\\to0}。 分数开方\\frac{ }{ }分数；\\sqrt{n}{r}表示开n次方。 括号和分割符() [] |是不变的；{}要转义，写为\\{\\}用\\left 和 \\right调整大小。 数学公式求和：\\sum_{i=0}^n{a_i}积分：\\int例子参见stackexchange","tags":[{"name":"技巧","slug":"jq","permalink":"http://abumaster.com/tags/jq/"}]},{"title":"图床测试","date":"2017-03-29T06:20:44.000Z","path":"2017/03/29/图床测试/","text":"图床一般是专门用来存储图片的服务器，同时向外提供链接。国内和国外之分，本次测试的是极简图床。优点：不必把图片上传到博客服务器，节省服务器的空间。缺点：不能上传大于5M的图片，稳定性待测。 来一波从wallhaven下载的图片","tags":[{"name":"技巧","slug":"jq","permalink":"http://abumaster.com/tags/jq/"}]},{"title":"Feedforward semantic segmentation with zoom-out features","date":"2017-03-27T06:21:29.000Z","path":"2017/03/27/Feedforward-semantic-segmentation-with-zoom-out-features/","text":"使用缩小特征的前馈语义分割 Feedforward semantic segmentation with zoom-out features 2015年CVPR论文，在PASCAL VOC 2012测试集上达到了69.9%的正确率。将小的图像元素（超像素）映射到丰富的特征表示中，这些特征是从嵌套的增加区域中获得。这些区域通过从超像素一直缩小到场景级别的分辨率获得。这种方法充分利用了图像和隐藏空间中的统计结构，而不显式设置结构化预测机制，从而避免了复杂、昂贵的推论。从而超像素是由多层前馈网络进行分类。 从大量的现代分割著作中，得到了一种被广泛接受的知识，分割可以看成一个结构化预测的任务，可以用条件随机场和结构化支持向量机模型。作者脱离传统，提出图像语义分割看作单阶段的分类任务，其中每个像素元素（超像素）被标记为一个标签，使用一个前馈模型，依据从图像计算的证据。用在前馈分类中的证据不是从孤立的局部区域中获得，而是从序列中获得，序列是怎么组成的呢？首先得到一个超像素，再向外扩展，获得一个更大的闭合区域，直到扩展到整张图片。计算每一个层次的丰富特征，结合所有特征，放入分类网络中。 ###缩小的特征融合将图像的类别分割转换成对一组超像素分类。由于我们期望为每个超级像素应用相同的分类机，我们希望超像素的性质是相似的，特别是它们的大小。使用了SLIC。本地超像素本身有很窄的范围，我们希望特征提取器可以捕获更多的本地信息：颜色，上下文，其他一些属性，在临近的超像素之间这些属性有很大的不同。近似距离场景","tags":[{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"},{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"}]},{"title":"全卷积网络用于图像语义分割","date":"2017-03-25T07:31:02.000Z","path":"2017/03/25/全卷积网络用于图像语义分割/","text":"全卷积网络用于图像语义分割 (Fully Convolutional Networks for Semantic Segmentation)[1] 全卷积网络实际上就是把普通卷积网络的最后的全连接层变为卷积层，因为全连接层会把空间信息隐藏，全部展开为一维向量，换为卷积可以保留空间信息。如VGG-16网络在处理ImageNet数据集时，最后的1000个输出，是1000维向量，来表示1000类事物的概率，当换为卷积层时，输出了1000个1*1大小的输出，对此上采样，可以输出对应的heat-map。这是分类网络作为稠密输出的关键。 文章解决的问题是如何生成稠密的预测即dense prediction Shift-and-stitch假设原图和FCN输出图之间的降采样因子f，对于原图的每个f*f区域，对于0 &lt;= x,y &lt;f处理这 f2 个输入，并且交替输出，使得预测在接受域的中心像素。每个像素对应一个中心像素，因此为稠密输出。缺点：感受野没变，但是原图被划分为了f*f大小的图像片作为输入图像，使得网络无法感受更加精细的信息。 稀疏过滤器调整下采样过程中的步长，变为1，可以保证下采样不会损失图像的大小。缺点：下采样的功能被减弱，同时保留了更多信息，接受域相对变小，可能损失全局信息，同样为卷积层带来了更多的运算。 上采样上采样（Upsampling）也称反卷积（Deconvolution），参数和卷积一样可以在训练中学习。运算也和卷积类似，为逆过程。设输入大小w0*h0，经过卷积后的大小为w1*h1，计算公式如下：卷积运算：w1 = (w0 + 2*pad - kernelsize)/stride + 1h1 = (h0 + 2*pad - kernelsize)/stride + 1反卷积运算：w0 = (w1 - 1)*stride + kernelsize - 2*padh0 = (h1 - 1)*stride + kernelsize - 2*pad经过上采样后的图像可能会比原图大，需要裁剪为原图像大小，caffe中的crop层，提供了很好的算法。 语义分割的框架结构文中提出的框架结构如图所示：作者发现32倍率的上采样导致输出图非常粗糙，因此想出了利用上层的一些特征来优化输出图像，就有了FCN-16s和FCN-8s的方案，其主要思想是利用上层的池化层的信息，减少上采样的倍率，保留了更多的特征。 具体的实践针对传统网络的全连接层变为卷积层，如VGG-16网络中第一个卷积层是25088*4096，将之解释为512*7*7*4096。产生端对端的训练模型。在论文提供的源码中，FCN-32s的配置文件，第一个卷积层为： layer { name: &quot;conv1_1&quot; type: &quot;Convolution&quot; bottom: &quot;data&quot; top: &quot;conv1_1&quot; param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0 } convolution_param { num_output: 64 pad: 100 #填充100 kernel_size: 3 stride: 1 } } 填充100的原因为：在VGG-16网络中卷积的参数，kernersize=3，stride=1，pad=1，所以卷积层不会改变图像的大小，所以图像只有在池化层改变大小，且变为原大小的一半。为了方便将图像看为一维的，设原图像大小h，经过了5层池化后，图像缩小了32倍，变为h5 = h/32，紧接着全连接层，可以看成是卷积层，卷积参数为：kernelsize=7 pad=0 stride=1，根据卷积计算公式，经过卷积层fc6后的输出图像大小为h6 = (h5-7)/1 + 1 = (h-192)/32 因此，图像小于192的就无法往下计算了，所以要pad=100，解决了网络输入图像固定大小的弊端，全卷积网络可以输入任意大小的图像。 例子根据FCN-32s的配置文件如果输入图像大小为3*320*320经过了卷积conv1的输出为：64*518*518经过了池化pool1的输出为：64*259*259经过了卷积conv2的输出为：128*259*259经过了池化pool2的输出为：128*130*130经过了卷积conv3的输出为：256*130*130经过了池化pool3的输出为：256*65*65经过了卷积conv4的输出为：512*65*65进过了池化pool4的输出为：512*32*32经过了卷积conv5的输出为：512*32*32经过了池化pool5的输出为：512*16*16经过了fc6的卷积后输出为：4096*10*10经过了fc7的卷积后输出为：4096*10*10经过score_fr的卷积输出：21*10*10上采样（反卷积）输出为：21*352*352score层裁剪后输出为：21*320*320","tags":[{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"},{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"}]},{"title":"卷积网络应该注意的问题","date":"2017-03-24T06:28:18.000Z","path":"2017/03/24/卷积网络应该注意的问题/","text":"卷积神经网络简介，由于其出色的特征提取特性，使得在计算机视觉方面有了很好的应用，并取得了出色的成绩。卷积卷积操作是卷积网络中的核心操作，其主要目的是为了提取图像的显著特征，降低特征维数，进而来减少计算量。在 caffe 代码中的主要参数如下： 123456789101112131415161718192021222324252627layer &#123; name: &quot;conv1&quot; type: &quot;Convolution&quot; bottom: &quot;data&quot; #上层是数据层 top: &quot;conv1&quot; param &#123; #权重学习参数 lr_mult: 1 #权重学习率 需要乘以基础学习率base\\_lr decay_mult: 1 &#125; param &#123; #偏置学习参数 lr_mult: 2 decay_mult: 0 &#125; convolution_param &#123; #卷积参数 num_output: 96 #卷积操作后的输出特征图 kernel_size: 11 #卷积核大小 stride: 4 #步长 #可能也有pad为扩充边缘 weight_filler &#123; #权值初始化 type: &quot;gaussian&quot; #类型为weight-filter 或xavier算法等，默认constant，全部0 std: 0.01 &#125; bias_filler &#123; #偏置的初始化 type: &quot;constant&quot; value: 0 &#125; &#125;&#125; 输入：n*c0*w0*h0输出：n*c1*w1*h2c1对应num_output，输出对应的大小计算:w1 = (w0 + 2*pad - kernersize)/stride + 1h1 = (h0 + 2*pad - kernelsize)/stride + 1在 caffe 源码中的计算是将图像和卷积核通过 im2col 转换成矩阵，再对两矩阵内积。 池化池化也称下采样，为了减少运算和数据维度的一种方式，被分为： 最大池化（Max Pooling），取最大值； 均值池化（Mean Pooling），取均值； 高斯池化。caffe 中的配置代码： 1234567891011layer &#123; name: &quot;pool1&quot; type: &quot;Pooling&quot; bottom: &quot;norm1&quot; top: &quot;pool1&quot; pooling_param &#123; #池化参数 pool: MAX #池化类型 kernel_size: 3 #池化核大小 stride: 2 #步长，重叠 &#125;&#125; 池化的计算公式与卷积操作类似：输入：n*c0*w0*h0输出：n*c1*w1*h2c1对应num_output，输出对应的大小计算:w1 = (w0 + 2*pad - kernersize)/stride + 1h1 = (h0 + 2*pad - kernelsize)/stride + 1 LRN层LRN全称为Local Response Normalization，即局部响应归一化层，没什么用，有一些网络中加入了这一层，对局部区域进行归一化，配置信息和参数说明如下：123456789101112layer &#123; name: &quot;norm1&quot; type: &quot;LRN&quot; bottom: &quot;conv1&quot; top: &quot;norm1&quot; lrn_param &#123; #参数 local_size: 5 #（1）通道间归一化时表示求和的通道数； #（2）通道内归一化时表示求和区间的边长； alpha: 0.0001 #缩放因子 beta: 0.75 #指数项 &#125;&#125; 激活函数 激活函数需要具有以下特性： 非线性； 单调、连续可微分； 范围不饱和，避免梯度为0； 原点近似线性。常用的激活函数有：Sigmoid 函数、Tanh 函数、ReLU 函数等。如 AlexNet 中用到的ReLU激活函数：$$f(x)=max(0,x)$$这种激活函数的特点是：无梯度损耗，收敛速度快，网络稀疏性大，计算量小。缺点是，梯度大的话，导致权重更新以后变大，输出0，使得神经元不再更新。因此要注意学习率的设置。 全连接层全连接层又称内积层（Inner-Product），是将特征图像全部展开为一维向量。caffe 中的文档显示： Inputn * c_i * h_i * w_i Outputn * c_o * 1 * 1这里引用了dupuleng的例子。lenet 网络配置文件中的一段： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647layers &#123; name: &quot;conv2&quot; type: CONVOLUTION bottom: &quot;pool1&quot; top: &quot;conv2&quot; blobs_lr: 1 blobs_lr: 2 convolution_param &#123; num_output: 50 kernel_size: 5 stride: 1 weight_filler &#123; type: &quot;xavier&quot; &#125; bias_filler &#123; type: &quot;constant&quot; &#125; &#125;&#125;layers &#123; name: &quot;pool2&quot; type: POOLING bottom: &quot;conv2&quot; top: &quot;pool2&quot; pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layers &#123; name: &quot;ip1&quot; type: INNER_PRODUCT bottom: &quot;pool2&quot; top: &quot;ip1&quot; blobs_lr: 1 blobs_lr: 2 inner\\_product\\_param &#123; num_output: 500 weight_filler &#123; type: &quot;xavier&quot; &#125; bias_filler &#123; type: &quot;constant&quot; &#125; &#125;&#125; conv2 的输入图像是256*27*27经过了卷积操作，输出50*22*22同样作为了pool2的输入，进行池化，pool2的输出50*11*11，下一层全连接层，输出500*1*1的向量，是如何进行计算的呢？要把所有通道全部展开做卷积，首先要把pool2输出的特征图展开为一维向量，共需要500*50*11*11个参数，进行卷积，输出500*1*1的一维向量。","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"caffe","slug":"caffe","permalink":"http://abumaster.com/tags/caffe/"}]},{"title":"Conditional Random Fields as Recurrent Neural Networks","date":"2017-03-21T08:00:18.000Z","path":"2017/03/21/CRFs-as-RNN/","text":"2015 年 ICCV 会议文章 Conditional Random Fields as Recurrent Neural Networks[1] 的阅读笔记。 关键词图像语义分割CRF as RNN摘要像素级别的标注任务，例如图像语义分割在图像理解方面占据着重要的作用。最近的方法开始利用深度学习技术在图像识别任务上的能力来解决像素级别的标注任务。现在的核心问题是深度学习方法在描绘可视化物体具有限制性。为了解决这个问题，我们提出了一个新形式的卷积网络，它结合了卷积网络的优势和条件随机场的概率图模型。为此，我们制定了使用高斯对模型和中值近似的条件随机场作为循环神经网络。这个网路就是 CRF-RNN 被嵌入到 CNN 中，最为一个集 CNNs 和 CRFs 优点于一体的深度网络。更重要的是，我们的系统完全在 CNNs 中集成了 CRF 模型，让使用传统的反向传播算法训练端对端的系统成为了可能，不需要额外的后期处理物体的边界。MarkDown 中使用公式 加入脚本定义，现在用到的是 MathJax 引擎12&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt; 使用Tex公式 $$行间公式；\\\\行内公式，参考MathJax basic tutorial and quick reference 示例$$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$ 引言低层次计算机视觉问题，为图像中的像素分配标签。特征表示在个体像素分类中占有重要的作用。同样要考虑到图像的边界和特征、空间关系，以此来获得较为准确地分割结果。设计出一个强大的特征表示器是像素级别标记的关键挑战。传统的方法不再讨论，现在深度学习的方法利用大尺度的卷积网络，在高层次视觉上取得了非常大的成果。这激励着利用卷积网络去解决低层次的问题。主要利用卷积网络提取特征替代以前的手工标注特征。将用于高层视觉的分类网络转换成低层次视觉的任务依然存在着一些问题提出了几个问题： 传统的卷积网络有大接受域的卷积过滤器，会产生比较粗糙的输出图。最大池化层的出现，过滤掉一些特征，导致了输出的分割图不够精细。 缺少了平滑度约束，没有考虑到相似的像素，空间或者外形相似的约束，导致了输出图的边界不明确，或者出现杂散区域。尤其是马尔科夫随机场（MRFs）和它的变体条件随机场（CRFs）已经成为应用到计算机视觉领域中一个成功的模型。用于像素标记的CRFs推理主要的思想是将语义标签分配问题转换成概率推理问题，包括了相似像素之间一致性并入假设。CRFs可以微调分割图的细节，优化边界问题，克服了单纯利用CNNs的缺点。用CRFs作为后期的处理，无法发挥出CRF的优势，卷积网络在训练的阶段也无法根据CRF的表现来调整权重。本文将CNN与CRF结合为一个统一的框架，可以共同训练。相关工作许多方法用深度学习来解决图像语义分割问题，可以归为以下两个类别： 特征提取和分割分离开的策略。使用CNN提取有意义的图像特征，利用超像素去构造图像的模式。首先从图像中获得超像素，再用特征提取器提取特征。存在着一个致命的缺点，前期如果有误差，后面误差越来越大。与他们的方案不同，此文用典型的图模型CRF可以被作为RNN，指定为深度网络的一部分。结合CNN实现端对端的训练。 直接学习从原始图像到标记图像的非线性模型。例如FCN等网络，去掉了最后的全连接层变为卷积层。全连接条件随机场 条件随机场进行图像语义分割的能量函数：定义隐变量Xi为像素点i的分类标签，取值范围为分类语义标签L={l1,l2,l3,…,ln}；Yi为每个随机变量Xi的观测值，即是每个像素的颜色值。条件随机场的目标就是通过观测变量Yi，推理出潜变量Xi的标签。对于一张图像，可以看成图模型G=(V,E)，每个顶点对应了V={X1,X2,...,Xn}，对于边来说，全连接的条件随机场，顶点与所有的点都有连线。条件随机场的目标函数：能量函数有一元势函数和二元势函数，分别表示了当像素点i的观测值是yi时，该像素点属于标签xi的概率。可以直接从cnn中计算出。二元是函数是两个像素值相似或者相邻则两个像素属于同一类的概率很大。实现 参考文献[1] Zheng S, Jayasumana S, Romera-Paredes B, et al. Conditional random fields as recurrent neural networks[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 1529-1537.","tags":[{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"},{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"}]},{"title":"柔性数组","date":"2017-03-12T13:59:48.000Z","path":"2017/03/12/柔性数组/","text":"C/C++中的0长数组 定义：柔性数组（Flexible Array）也叫伸缩性数组、变长数组。 作用 ：放入结构体中，可以存放动态长度的字符串、数组等。 用法举例：放在结构体的最后，长度为0的数组。长度为0不占用任何空间，数组名只是一个符号，代表了一个不可改变的地址。 1234struct package &#123; int len; char data[0];&#125;; 用途：根据变长数组的特性很容易构造出一些数据结构，缓冲区、数据包等。不会浪费多余的空间，用多少申请多少。 使用:假设用上面的结构来发送1024字节大小的数据包，首先要构造一个数据包：1234char *pMsg = (char *)malloc(sizeof(package)+1024); package *pPack = (package*)pMsg;pPack-&gt;len = 1024;memcpy(pPack-&gt;data, source, 1024); 强制类型转换，将package类型的指针指向了申请的内存的开始，分为两个部分：前一部分表示字符串的长度，后一部分表示实际的内容。将整个数据包发出去，不会浪费一点额外的空间，在网络中传输节省了流量，提升了速度。","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]}]