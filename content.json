[{"title":"UNIX环境高级编程-进程","date":"2017-05-19T02:22:59.000Z","path":"2017/05/19/UNIX环境高级编程-进程/","text":"Unix环境高级编程，第7、8、9章有关进程的读书笔记。 进程环境进程的启动和终止。一般c程序从入口函数main开始，调用一系列用户函数等；进程终止有正常终止和异常终止两种，其中，正常终止是程序从main函数返回(return)，或者正常调用退出函数exit _exit _Exit，或者线程的返回或退出，异常终止通常会调用abort或者信号中断。命令行参数和环境表它们都是有外界给程序的参数，就像标注的 ISO C 规定的主函数书写格式 main(int argc,char *argv[]```，将命令行参数保存，而环境变量表则被取消了，仍可以通过函数访问环境表。 12345678910111213141516171819202122查看和设置环境变量， ```c#include &lt;stdlib.h&gt;char *getenv(const char* envname);int putenv(char *str);//不会分配空间int setenv(char *name, char *value, int rewrite);//分配空间int unsetenv(const char *name);``` **C 程序的存储空间分布** 如图： ![](http://oo7zsi4t8.bkt.clouddn.com/17-5-19/39758826-file_1495161867318_14f4a.png) + 正文段，存放程序运行所需的机器指令部分，具有只读属性； + 初始化数据段，明确初始化的变量；+ 未初始化的数据； + 栈，存放自动变量或者临时变量； + 堆，动态分配的空间。 **非局部goto** ```c#include &lt;setjmp.h&gt;int setjmp(jmp_buf env);//设置返回的位置void longjmp(jmp_buf env, int val);//开始返回 回滚一些变量的值，如果不想回滚到之前的值，可以将变量定义为：volatile，全局或静态变量的值也会保持不变。 进程控制进程标识符非负整型来表示唯一进程ID，但是可以重用，通常ID=0表示交换进程或者调度进程，ID=1表示init进程，ID=2表示页守护进程。123#include &lt;unistd.h&gt;pid_t getpid();//进程IDpid_t getppid();//父进程ID fork函数12#include &lt;unistd.h&gt;pid_t fork(void); 两个返回，子进程返回0，通过getppid获得父进程id，ID为0，是交换进程使用，父进程返回子进程的ID，因为，这是父进程获得子进程ID的唯一方式。exec函数fork创建新进程，exec可以执行新程序，exit处理终止，wait等待终止。","tags":[{"name":"Linux","slug":"linux","permalink":"http://abumaster.com/tags/linux/"}]},{"title":"UNIX环境高级编程-标准IO库","date":"2017-05-18T00:39:42.000Z","path":"2017/05/18/UNIX环境高级编程-标准IO库/","text":"Unix环境高级编程读书笔记，第5章 标准I/O库。引入流的概念，并引入缓冲，减少read和write的调用次数。 1.文件I/O和标准I/O区别标准I/O使用了缓冲机制，文件I/O不使用，而是直接调用内核中的一个系统调用完成。操作的对象不同，文件io操作的是文件描述符，标准io操作的是流，流与磁盘等外围设备关联。他们的函数对比。 操作 标准I/O 文件I/O 打开 fopen, froen, fdopen open 关闭 fclose close 读 getc, fgetc, getchar,fgets, gets,fread read 写 putc, fputc, putchar,fputs, puts,fwrite write 2.缓冲 标准I/O提供的几种缓冲及其区别。 全缓冲，填满缓冲区后再进行实际的I/O操作，磁盘文件通常使用全缓冲。填满缓冲区后，调用fflush来刷新缓冲区，flush(冲洗)用来将缓冲区的内容写到磁盘上；flush(刷清)丢弃已经存储在缓冲区中的数据，用在终端驱动程序方面。 行缓冲，用在终端的输入输出，遇到换行符的时候，或者缓冲区满。 无缓冲，标准出错流，stderr，错误信息及时显示出来。 对于一个打开的流，设置更改缓冲区。 12345678#include &lt;stdio.h&gt;void setbuf(FILE *restrict fp, char *restrict buf);void setvbuf(FILE *restrict fp, char *restrict buf, int mode, size_t size);/* mode 参数： _IOFBF 全缓冲；_IOLBF 行缓冲；_IONBF 不带缓冲 *///强制冲洗流int fflush(FILE *fp); 3.操作打开关闭流打开关闭标准I/O流，如上表的函数所示。读写流打开了流，有三种类型的方式进行读写操作： 每次一个字符的I/O; 每次一行的I/O; 直接I/O。 每个流在FILE对象上维持了两个标志：出错标志，文件结束标志。gets 和fgets，不推荐使用前者，因为不能指定缓冲区大小，容易造成缓冲区溢出，另外，gets不保留换行符。4.临时文件123#include&lt;stdio.h&gt;char* tmpnam(char *ptr);//指向唯一路径的指针char *tmpfile(void); //返回文件指针","tags":[{"name":"Linux","slug":"linux","permalink":"http://abumaster.com/tags/linux/"},{"name":"c","slug":"c","permalink":"http://abumaster.com/tags/c/"},{"name":"编程","slug":"编程","permalink":"http://abumaster.com/tags/编程/"}]},{"title":"UNIX环境高级编程-文件和目录","date":"2017-05-16T11:39:00.000Z","path":"2017/05/16/UNIX环境高级编程-文件和目录/","text":"UNIX环境高级编程读书笔记，第4章 文件和目录。 I/O操作描述的是普通文件的读写等操作，本章介绍文件系统的其他特征和文件的性质。 1.三个stat函数1234#include &lt;sys/stat.h&gt;int stat(const char *restrict pathname, struct stat *restrict buf);int fstat(int filedes, struct stat *buf);int lstat(const char *restrict pathname, struct stat *restrict buf); 返回与此文件相关联的信息结构stat，关于stat的结构说明：123456789101112131415struct stat &#123; mode_t st_mode;//文件类型 ino_t st_ino;//i节点 dev_t st_dev;//设备号 文件系统 dev_t st_rdev; nlink_t st_nlink; uid_t st_uid; gid_t st_gid; off_t st_size;//大小 time_t st_atime;//访问时间 time_t st_mtime;//修改时间 time_t st_ctime;//改变时间 blksize_t st_blksize;//块大小 blkcnt_t st_blocks;//分配的磁盘块&#125; 2.文件类型 普通文件； 目录文件； 块特殊文件； 字符特殊文件； FIFO，进程间通信，命名管道； 套接字，网络间通信； 符号链接在&lt;sys/stat.h&gt;中定义了获取文件类型的宏，参数为st_mode成员。 宏 文件类型 S_ISREG() 普通文件 S_ISDIR() 目录文件 S_ISCHR() 字符特殊文件 S_ISBLK() 块特殊文件 S_ISFIFO() 管道或FIFO S_ISLNK() 符号链接 S_ISSOCK() 套接字 3.文件的访问权限分为三类：用户、组、其他，而每类对应的权限为：读、写、执行。4.文件系统i节点：固定长度的记录项，保存着文件的大部分信息。理解inode。内核中，以inode编号来标识文件，而不是以文件名标识文件。","tags":[{"name":"Linux","slug":"linux","permalink":"http://abumaster.com/tags/linux/"}]},{"title":"UNIX环境高级编程-文件IO","date":"2017-05-15T12:29:43.000Z","path":"2017/05/15/UNIX环境高级编程-文件IO/","text":"UNIX环境高级编程读书笔记，第3章 文件 I/O。 文件描述符Linux一切皆文件，无论是设备还是文档都是一个文件，这种抽象显示了Linux系统的灵活和通用性。文件描述符一般是一个非负整数，当打开或者创建一个文件时，内核向进程返回一个文件描述符，此描述符用于其他操作的参数。通常在unistd.h中定义了常量：STDIN_FILENO STDOUT_FILENO STDERR_FILENO分别代表数字0,1,2是标准输入输出错误输出三种基本的文件描述符。open函数描述：打开或者创建文件，返回文件描述符或者-112#include &lt;fcntl.h&gt;int oepn(const char* pathname, int flags,.../*mode_t mode*/); creat函数描述：创建一个文件，只能只写的方式打开，成功返回文件描述符，失败-1close函数描述：关闭文件描述符lseek函数描述：为一个打开的文件描述符设置偏移量，成功返回新的文件偏移量，失败-112#include &lt;unistd.h&gt;off_t lseek(int filedes, off_t offset, int whence); 偏移量的方式取决于第三个参数。SEEK_SET 开始处设置偏移量，绝对偏移量；SEEK_CUR 当前位置设置偏移量，相对偏移量；SEEK_END 结束开始设置偏移量，相对于文件末端偏移量。read函数打开的文件中读取数据，返回读取的字节数，如果剩余文件不够要读的字节数。write函数向打开的文件中写数据，返回实际写入的数据字节数。 文件共享不同进程之间共享打开的文件，内核使用三种数据结构表示打开的文件。1.进程表项描述一个打开的文件描述表，每个文件描述符表包含了两项： 文件描述符标志 指向文件表项的指针2.文件表每个文件表包含如下信息： 文件状态标志（读写…) 当前文件偏移量 指向文件v节点表项的指针3.v节点表v节点表表示文件类型，以及对文件进行各种操作的指针，也包含了i节点及文件长度等信息。不同进程打开同一个文件的各项状态。 dup和dup2函数用来复制一个现存的文件描述符。123#include &lt;unistd.h&gt;int dup(int filedes);int dup2(int filedes, int filedes2); dup返回的文件描述符是当前可用文件描述符的最小值；dup2可以用filedes来指定新的描述符，如果filedes2已经打开，则先将其关闭，如果相等，则返回filedes2，不必关闭。执行dup后，文件表项和v节点表项不变。作用：一般用于重定向和共享文件，如父进程处理了一些文件，现在需要子进程处理，可以dup一份；同样dup2的使用，可以看为dup2(源, 目标)，目标将会被源替换掉。具体使用。fcntl函数可以读取和改变打开文件的性质。12#include &lt;fcntl.h&gt;int fcntl(int filedes, int cmd, .../*int arg*/); fnctl的功能： 复制一个现有的描述符，cmd=F_DUPFD 获取/设置文件描述符标记(cmd=F_GETFD / F_SETFD) 获得和设置文件状态标志cmd=F_GETFL F_SETFL 获取和设置异步I/O所有权cmd=F_GETOWN F_SETOWN 获得和设置记录锁cmd = F_GETLK F_SETLK同样，复制文件描述符函数：dup(filedes)等价于fcntl(filedes, F_DUPFD, 0)。调用dup2(filedes1,filedes2)相当于调用：close(filedes2); fcntl(filedes1, F_DUPFD, filedes2)。不同之处在于dup2函数是原子操作，而用fcntl是两个函数调用。dup2的功能:12345dup2(fd, 0);dup2(fd, 1);dup2(fd, 2);if(fd &gt; 2) close(fd); 假设fd=1，则执行后的结果图如下：fd=3时，结果同，把3的文件复制到前3个上，删除以后的。","tags":[{"name":"Linux","slug":"linux","permalink":"http://abumaster.com/tags/linux/"}]},{"title":"High-performance Semantic Segmentation using VDFC","date":"2017-05-10T07:37:56.000Z","path":"2017/05/10/High-performance-Semantic-Segmentation-using-VDFC/","text":"High-performance Semantic Segmentation Using Very Deep Fully Convolution [1] ，论文阅读笔记。 本文做的贡献： 探索不同的全卷积残差网络找到更好的配置，诸如，网络的层数、特征图的分辨率、感受野的大小等，由于内存的限制等因素，提出了用低分辨率网络来模拟高分辨网络进行训练和测试； 提出了在线引导（online booststrapping）的方法进行训练，已经论证可以达到更好的正确率； 将传统的dropout应用到残差块中； 达到了很好的结果。 1.低分辨率近似高分辨率的模型由于内存的限制，网络不允许输入过大分辨率的图像，但是分辨率大的图像往往可以保存更多的细节信息，可以达到更好的分割效果，所以，提出了这个低分辨率来近似高分辨率的模型。基本的做法是：如果输入一个图像，经过了中间的若干层，图像的分辨率会下降，假设缩小为原始的1/8，（1）产生了一个1/8的特征图，这时，（2）可以在上一层池化层提取出剩下1/8的图像，（3）分别获得两个1/8的得分图，（4）组合，得到1/4的得分图或者是标签。 2.损失函数$$ e = -\\frac{1}{\\sum_i^N \\sum_j^K{1\\{y_i=j\\ and\\ p_{ij}&lt;t\\}}}(\\sum_i^N\\sum_j^K1\\{y_i=j \\ and\\ p_{ij}&lt;t\\}logp_{ij})$$ [1] Wu Z, Shen C, Hengel A. High-performance semantic segmentation using very deep fully convolutional networks[J]. arXiv preprint arXiv:1604.04339, 2016. K表示语义标签，N表示像素的个数，$p_{ij}$表示像素$a_i$分到标签$c_j$的概率，$y_i$表示$a_i$的正确标签。符号$1{.}$表示满足括号里的条件为1，不满足为0。","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"计算机视觉","slug":"computerversion","permalink":"http://abumaster.com/tags/computerversion/"}]},{"title":"宏定义","date":"2017-05-09T12:36:26.000Z","path":"2017/05/09/宏定义/","text":"宏定义进入编译器之前展开替换。 宏常量#define MAX 100用100替换符号MAX，c++中一般不推荐使用，通常用常量const定义；用于条件编译的宏如避免包含重复头文件的宏： 12345#ifdefine XXX #define XXX //some include file#endif` 宏函数避免函数调用，提高执行效率，以空间换取时间。对于一些重复的函数可以声明为宏函数，就像内联函数一样…例子： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667typedef int (*Onefunction)();//定义函数指针typedef map&lt;string, Onefunction&gt; OneMap;//名称，函数指针相关联的mapOneMap g_one_map;//全局变量保存//注册函数的宏，其中展开为一个按名定义的类，//构造函数,将函数地址和函数名称放入全局的map中//最后一个简单的类对象声明，可以保证构造函数的执行，//作用域可以保证在执行完后对象的销毁，用过即销毁。#define RegisterOneFunction(func) \\&#123; \\class __Register_##func &#123; \\public: \\__Register_##func() &#123; \\g_one_map[#func] = &amp;func; \\&#125; \\&#125;; \\__Register_##func g_register_##func; \\&#125;//自定义的函数，无参，返回intint func1()&#123; cout &lt;&lt; \"func1 out...\\n\"; return 0;&#125;int func2()&#123; cout &lt;&lt; \"func2 out...222\\n\"; return 0;&#125;//调用宏，注册函数void WrapperRegisterFunction()&#123; RegisterOneFunction(func1); RegisterOneFunction(func2);&#125;//根据函数名称获得函数的指针Onefunction GetOneFunction(const string&amp; fname)&#123; if(g_one_map.count(fname)) &#123; return g_one_map[fname]; &#125; else &#123; cout &lt;&lt; \"not found\"&lt;&lt;endl; for(OneMap::iterator it=g_one_map.begin(); it!=g_one_map.end(); it++) &#123; cout &lt;&lt;it-&gt;first&lt;&lt;\" \"; &#125; cout&lt;&lt;endl; return NULL; &#125;&#125;int main()&#123; string funNmae; WrapperRegisterFunction(); cin &gt;&gt; funNmae; //以名称来使用函数 GetOneFunction(funNmae)(); return 0;&#125; 注意事项 1.普通宏定义 宏名一般用大写 使用宏可提高程序的通用性和易读性，减少不一致性，减少输入错误和便于修改 预处理是在编译之前的处理，而编译工作的任务之一就是语法检查，预处理不做语法检查 宏定义末尾不加分号 宏定义写在函数的花括号外边，作用域为其后的程序，通常在文件的最开头 可以用#undef命令终止宏定义的作用域 宏定义可以嵌套 字符串””中永远不包含宏 宏定义不分配内存，变量定义分配内存2.带参宏定义 实参如果是表达式容易出问题 宏名和参数的括号间不能有空格 宏替换只作替换，不做计算，不做表达式求解 函数调用在编译后程序运行时进行，并且分配内存。宏替换在编译前进行，不分配内存 宏的哑实结合不存在类型，也没有类型转换 函数只有一个返回值，利用宏则可以设法得到多个值 宏展开使源程序变长，函数调用不会 宏展开不占运行时间，只占编译时间，函数调用占运行时间（分配内存、保留现场、值传递、返回值）","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]},{"title":"caffe学习-分类","date":"2017-05-07T11:29:00.000Z","path":"2017/05/07/‘caffe学习-分类/","text":"在用caffe的c++接口时，遇到了许多问题，学习源码中解决问题，熟悉一些细节。 1.预测分类的流程图 2.代码注释123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239//classification.cpp/*一些头文件*/#ifdef USE_OPENCV/* Pair (label, confidence) * 代表一个预测结果，标签和概率的组合 */typedef std::pair&lt;string, float&gt; Prediction;//定义一个分类的类class Classifier &#123; public: Classifier(const string&amp; model_file, const string&amp; trained_file, const string&amp; mean_file, const string&amp; label_file);//提供给外部的接口，返回一个预测。参数：需要预测的图像和概率最大的N个结果 std::vector&lt;Prediction&gt; Classify(const cv::Mat&amp; img, int N = 5); private: void SetMean(const string&amp; mean_file);//设置中值 std::vector&lt;float&gt; Predict(const cv::Mat&amp; img); void WrapInputLayer(std::vector&lt;cv::Mat&gt;* input_channels); void Preprocess(const cv::Mat&amp; img, std::vector&lt;cv::Mat&gt;* input_channels); private: shared_ptr&lt;Net&lt;float&gt; &gt; net_; cv::Size input_geometry_; int num_channels_; cv::Mat mean_; std::vector&lt;string&gt; labels_;&#125;;Classifier::Classifier(const string&amp; model_file, const string&amp; trained_file, const string&amp; mean_file, const string&amp; label_file) &#123;#ifdef CPU_ONLY Caffe::set_mode(Caffe::CPU);#else Caffe::set_mode(Caffe::GPU);#endif //加载网络配置并初始化 net_.reset(new Net&lt;float&gt;(model_file, TEST)); net_-&gt;CopyTrainedLayersFrom(trained_file); CHECK_EQ(net_-&gt;num_inputs(), 1) &lt;&lt; \"Network should have exactly one input.\"; CHECK_EQ(net_-&gt;num_outputs(), 1) &lt;&lt; \"Network should have exactly one output.\"; //取出输入层的blob结构，可以提取出通道和输入图像的高宽 Blob&lt;float&gt;* input_layer = net_-&gt;input_blobs()[0]; num_channels_ = input_layer-&gt;channels(); CHECK(num_channels_ == 3 || num_channels_ == 1) &lt;&lt; \"Input layer should have 1 or 3 channels.\"; input_geometry_ = cv::Size(input_layer-&gt;width(), input_layer-&gt;height()); //加载中值文件 SetMean(mean_file); //加载标签文件 std::ifstream labels(label_file.c_str()); CHECK(labels) &lt;&lt; \"Unable to open labels file \" &lt;&lt; label_file; string line; while (std::getline(labels, line)) labels_.push_back(string(line)); //检查标签数目和输出维度是否匹配 Blob&lt;float&gt;* output_layer = net_-&gt;output_blobs()[0]; CHECK_EQ(labels_.size(), output_layer-&gt;channels()) &lt;&lt; \"Number of labels is different from the output layer dimension.\";&#125;//自定义比较函数，用于排序预测结果static bool PairCompare(const std::pair&lt;float, int&gt;&amp; lhs, const std::pair&lt;float, int&gt;&amp; rhs) &#123; return lhs.first &gt; rhs.first;&#125;//返回v中元素最大的N个数的下标索引static std::vector&lt;int&gt; Argmax(const std::vector&lt;float&gt;&amp; v, int N) &#123; std::vector&lt;std::pair&lt;float, int&gt; &gt; pairs; for (size_t i = 0; i &lt; v.size(); ++i) pairs.push_back(std::make_pair(v[i], static_cast&lt;int&gt;(i))); std::partial_sort(pairs.begin(), pairs.begin() + N, pairs.end(), PairCompare); std::vector&lt;int&gt; result; for (int i = 0; i &lt; N; ++i) result.push_back(pairs[i].second); return result;&#125;//输入图像，返回前N个概率最大的预测(标签，概率)std::vector&lt;Prediction&gt; Classifier::Classify(const cv::Mat&amp; img, int N) &#123; std::vector&lt;float&gt; output = Predict(img); N = std::min&lt;int&gt;(labels_.size(), N); std::vector&lt;int&gt; maxN = Argmax(output, N); std::vector&lt;Prediction&gt; predictions; for (int i = 0; i &lt; N; ++i) &#123; int idx = maxN[i]; predictions.push_back(std::make_pair(labels_[idx], output[idx])); &#125; return predictions;&#125;/* Load the mean file in binaryproto format. */void Classifier::SetMean(const string&amp; mean_file) &#123; BlobProto blob_proto; ReadProtoFromBinaryFileOrDie(mean_file.c_str(), &amp;blob_proto); /* Convert from BlobProto to Blob&lt;float&gt; */ Blob&lt;float&gt; mean_blob; mean_blob.FromProto(blob_proto); CHECK_EQ(mean_blob.channels(), num_channels_) &lt;&lt; \"Number of channels of mean file doesn't match input layer.\"; /* The format of the mean file is planar 32-bit float BGR or grayscale. */ std::vector&lt;cv::Mat&gt; channels; float* data = mean_blob.mutable_cpu_data(); for (int i = 0; i &lt; num_channels_; ++i) &#123; /* Extract an individual channel. */ cv::Mat channel(mean_blob.height(), mean_blob.width(), CV_32FC1, data); channels.push_back(channel); data += mean_blob.height() * mean_blob.width(); &#125; /* Merge the separate channels into a single image. */ cv::Mat mean; cv::merge(channels, mean); /* Compute the global mean pixel value and create a mean image * filled with this value. */ cv::Scalar channel_mean = cv::mean(mean); mean_ = cv::Mat(input_geometry_, mean.type(), channel_mean);&#125;//预测函数，返回输出的概率std::vector&lt;float&gt; Classifier::Predict(const cv::Mat&amp; img) &#123; Blob&lt;float&gt;* input_layer = net_-&gt;input_blobs()[0]; input_layer-&gt;Reshape(1, num_channels_, input_geometry_.height, input_geometry_.width); /* Forward dimension change to all layers. */ net_-&gt;Reshape(); std::vector&lt;cv::Mat&gt; input_channels; WrapInputLayer(&amp;input_channels); Preprocess(img, &amp;input_channels); net_-&gt;Forward(); /* Copy the output layer to a std::vector */ Blob&lt;float&gt;* output_layer = net_-&gt;output_blobs()[0]; const float* begin = output_layer-&gt;cpu_data(); const float* end = begin + output_layer-&gt;channels(); return std::vector&lt;float&gt;(begin, end);&#125; /* 包装网络的输入层，将每个通道保存为Mat对象， * 最后直接将分割的通道写入到输入层中 */void Classifier::WrapInputLayer(std::vector&lt;cv::Mat&gt;* input_channels) &#123; Blob&lt;float&gt;* input_layer = net_-&gt;input_blobs()[0]; int width = input_layer-&gt;width(); int height = input_layer-&gt;height(); //获取可更改的输入层数据指针 float* input_data = input_layer-&gt;mutable_cpu_data(); for (int i = 0; i &lt; input_layer-&gt;channels(); ++i) &#123; cv::Mat channel(height, width, CV_32FC1, input_data); input_channels-&gt;push_back(channel); input_data += width * height; &#125;//将各个通道变为Mat，依次放入vector中&#125;//图像拷贝入输入层中void Classifier::Preprocess(const cv::Mat&amp; img, std::vector&lt;cv::Mat&gt;* input_channels) &#123; /* 将输入图像转换为网络要求的输入格式 */ //通道数 cv::Mat sample; if (img.channels() == 3 &amp;&amp; num_channels_ == 1) cv::cvtColor(img, sample, cv::COLOR_BGR2GRAY); else if (img.channels() == 4 &amp;&amp; num_channels_ == 1) cv::cvtColor(img, sample, cv::COLOR_BGRA2GRAY); else if (img.channels() == 4 &amp;&amp; num_channels_ == 3) cv::cvtColor(img, sample, cv::COLOR_BGRA2BGR); else if (img.channels() == 1 &amp;&amp; num_channels_ == 3) cv::cvtColor(img, sample, cv::COLOR_GRAY2BGR); else sample = img; //大小 cv::Mat sample_resized; if (sample.size() != input_geometry_) cv::resize(sample, sample_resized, input_geometry_); else sample_resized = sample; //浮点数 cv::Mat sample_float; if (num_channels_ == 3) sample_resized.convertTo(sample_float, CV_32FC3); else sample_resized.convertTo(sample_float, CV_32FC1); //归一化处理：减去中值 cv::Mat sample_normalized; cv::subtract(sample_float, mean_, sample_normalized); //直接将mat拷贝到输入层，已经处理过输入层为Mat对象了 cv::split(sample_normalized, *input_channels); CHECK(reinterpret_cast&lt;float*&gt;(input_channels-&gt;at(0).data) == net_-&gt;input_blobs()[0]-&gt;cpu_data()) &lt;&lt; \"Input channels are not wrapping the input layer of the network.\";&#125;//主函数命令行调用int main(int argc, char** argv) &#123; if (argc != 6) &#123; std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" deploy.prototxt network.caffemodel\" &lt;&lt; \" mean.binaryproto labels.txt img.jpg\" &lt;&lt; std::endl; return 1; &#125; ::google::InitGoogleLogging(argv[0]); string model_file = argv[1]; string trained_file = argv[2]; string mean_file = argv[3]; string label_file = argv[4]; Classifier classifier(model_file, trained_file, mean_file, label_file); string file = argv[5]; std::cout &lt;&lt; \"---------- Prediction for \" &lt;&lt; file &lt;&lt; \" ----------\" &lt;&lt; std::endl; cv::Mat img = cv::imread(file, -1); CHECK(!img.empty()) &lt;&lt; \"Unable to decode image \" &lt;&lt; file; std::vector&lt;Prediction&gt; predictions = classifier.Classify(img); /* Print the top N predictions. */ for (size_t i = 0; i &lt; predictions.size(); ++i) &#123; Prediction p = predictions[i]; std::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(4) &lt;&lt; p.second &lt;&lt; \" - \\\"\" &lt;&lt; p.first &lt;&lt; \"\\\"\" &lt;&lt; std::endl; &#125;&#125;#elseint main(int argc, char** argv) &#123; LOG(FATAL) &lt;&lt; \"This example requires OpenCV; compile with USE_OPENCV.\";&#125;#endif // USE_OPENCV","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]},{"title":"Caffe笔记","date":"2017-05-01T06:41:52.000Z","path":"2017/05/01/Caffe笔记/","text":"Caffe学习中的遇到的一些问题拾遗。 1..solverstate的使用在网络训练过程中当保存一个快照时，会保存两个文件：**.caffemodel 和 **.solverstate 第一个文件是训练过程中，迭代了N次，保存的模型，第二个文件是训练过程意外暂停，如ctrl+C 或者电脑死机，保存的网络状态，下一次网络可以接着训练，参考Caffe Wiki - Training and Resuming。使用： 命令行训练：caffe train -solver solver.prototxt状态中恢复训练：caffe train -solver solver.prototxt -snapshot train_190000.solverstate Python 接口从模型中copy参数：1234weights = '../ilsvrc-nets/vgg16-fcn.caffemodel'# initsolver = caffe.SGDSolver('solver.prototxt')solver.net.copy_from(weights) 从状态中恢复训练： 12solver = caffe.SGDSolver('solver.prototxt')solver.restore('snapshot/train_iter_2000.solverstate') 这时不需要copy参数了。 2.编写网络配置文件通常创建一个创建一个 solver 来表示网络的参数信息，包括：迭代次数，训练策略以及保存快照等。其中包含了一个训练网络模型定义和一个测试网络模型定义文件，也可以写在一个配置文件中，当写在一个文件中的时候，要在网络的不同之处加上： 123include &#123; phase: TEST (TRAIN) &#125; 3.网络运行过程 加载 solver 有两种方式（Python 接口）：solver = caffe.get_solver(&#39;models/bvlc_reference_caffenet/solver.prototxt&#39;) 和solver = caffe.SGDSolver(&#39;models/bvlc_reference_caffenet/solver.prototxt&#39;) 开始训练： 12solver.net.forward() # train netsolver.test_nets[0].forward() # test net (there can be more than one) 这是一次从输入层到损失层的计算过程，最后计算出loss，反向传播时，可以写为：solver.net.backward()，这是计算从损失层到输入层的梯度，并更新网络中各层的参数信息。前向传播和反向传播可以合并写，表示一次完整的计算：solver.step(1)。如果要按照配置文件中的最大迭代次数运行网络，则写为：solver.solve()。 4.验证模型正确率12345678accuracy = 0batch_size = solver.test_nets[0].blobs['data'].num #训练批次test_iters = int(len(Xt) / batch_size) #迭代次数for i in range(test_iters): solver.test_nets[0].forward() #测试网络 accuracy += solver.test_nets[0].blobs['accuracy'].data #相加每次迭代的正确率accuracy /= test_iters #平均正确率print(\"Accuracy: &#123;:.3f&#125;\".format(accuracy)) 5.定义自己的Python层Python层通常用来对输入数据进行预处理，如在图像语义分割中，输入为Python层，用于读取训练图像和分割图像。自定义Python层是，需在prototxt文件中指明层的类型为python并且指明需要的函数，如：1234567891011layer &#123; name: 'MyPythonLayer' type: 'Python' top: 'output' bottom: 'conv' python_param &#123; module: 'mypythonlayer' layer: 'MyLayer' param_str: \"'num': 21\" &#125;&#125; 然后，需要按以下格式定义自己的Python文件，如：12345678910111213141516171819import caffeimport numpy as npimport yamlclass MyLayer(caffe.Layer): def setup(self, bottom, top): self.num = yaml.load(self.param_str)[\"num\"] print \"Parameter num : \", self.num def reshape(self, bottom, top): pass def forward(self, bottom, top): #前传 top[0].reshape(*bottom[0].shape) top[0].data[...] = bottom[0].data + self.num def backward(self, top, propagate_down, bottom): pass 使用时还与普通网络调用一样进行，只是会直接用python定义的层完成输入数据的重新组织，再进行传递。","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"技巧","slug":"jq","permalink":"http://abumaster.com/tags/jq/"}]},{"title":"尺度感知模型","date":"2017-04-25T07:02:25.000Z","path":"2017/04/25/尺度感知模型/","text":"来自 2016 年 ICCV 论文：Attention to Scale: Scale-aware Semantic Image Segmentation，注意尺度：尺度敏感图像语义分割。在全卷积网络中合并多尺度特征已经是提高图像语义分割效果的一个关键因素。通过不同图像尺寸的输入，提取出不同尺度的信息，通过一个注意力模型获得权重融合特征图，最终得到分割图像。 1.注意力模型 Attention modelAttention model(AM)最先在计算机视觉中被应用于图片识别的问题，之后在自然语言处理(NLP)和计算机视觉(CV)中经常结合递归神经网络结构RNN、GRU、LSTM等深度学习算法，被称之为Recurrent Attention Model(RAM)，其核心就是一个Encoder-Decoder的过程。图像识别中，经常把图像缩放成固定大小，引起信息的丢失，结合人看物体时，目光会沿着感兴趣的方向移动，甚至聚焦感兴趣的区域，Attention（注意力）就是在网络中加入关注区域的移动、缩放机制、连续部分信息序列化输入。知乎问题回答。可以分为两种模型： hard：Attention 每次移动固定大小区域； soft：Attention 每次是所有区域的一个加权和。注意力，人看一副图像不是按像素点去看的，往往是一个区域一个区域看的，关注感兴趣区域（Region of Interest），Attention 可以自动寻找感兴趣的区域？强化学习。2.如何利用多尺度信息在 FCNs 场景下，有两种方式利用多尺度信息，如图所示： 利用网络中间层信息，由于随着网络层数的增加，图像不断缩小，图像的特征也会不断地丢失，在经典的FCN-8s网络中提出了融合中间层的特征图，优化最后的分割结果； 多尺度图像输入，网络共享权重，不同尺度会产生不同大小的特征图，每个尺度的特征图关注点也不同，通过在最后对不同尺度特征图的融合，产生最终的分割结果。3.模型介绍模型如何运作？不同尺度图像输入 FCNs 中会生成不同的热力图（得分图），然后，如何融合不同的得分图，论文中提出了一个注意力模型，对于每个尺度特征图输出一个权重图，权重是如何生成的呢，通过学习，对于大物体在褚略的特征图上置为较大的权重。有了权重图，结合特征图，很容易加权融合多个尺度特征图，得到最后的输出。每个尺度，对应着一个score map，这里乘以由尺度获得的权重图，得到最终的输出图。权重是由注意力模型产生的score map的所占比重决定的，表示摸个尺度的重要性。","tags":[{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"},{"name":"计算机视觉","slug":"computerversion","permalink":"http://abumaster.com/tags/computerversion/"}]},{"title":"零散","date":"2017-04-19T06:25:45.000Z","path":"2017/04/19/零散/","text":"VMware虚拟机在Windows下错误 出现如下错误：VMware Workstation and Device/Credential Guard are not compatible Windows的虚拟化技术Hyper-v和VMware的虚拟化技术不兼容的问题！解决方案：1.关闭hyper-v服务如图，关闭红色框内的功能。2.增加开机启动选项stackoverflow 问题回答在CMD管理员身份运行，注意不能用PowerShell。编辑bcd。12345C:\\&gt;bcdedit /copy &#123;current&#125; /d &quot;No Hyper-V&quot; The entry was successfully copied to &#123;ff-23-113-824e-5c5144ea&#125;. C:\\&gt;bcdedit /set &#123;ff-23-113-824e-5c5144ea&#125; hypervisorlaunchtype off The operation completed successfully. 重启经过一段时间配置，开机启动项出现了两个选项： Windows 10 No Hyper-V 删除开启启动项：在CMD中输入：123C:\\&gt;bcdedit /v列出了开机启动项，删除对应的选项即可。C:\\&gt;bcdedit /delete &#123;ff-23-113-824e-5c5144ea&#125; 另一种，在系统配置中直接配置引导项。 图像分类： Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.Papandreou G, Kokkinos I, Savalle P A. Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 390-399. 物体检测： Girshick R, Donahue J, Darrell T, et al. Rich feature hierarchies for accurate object detection and semantic segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2014: 580-587.Erhan D, Szegedy C, Toshev A, et al. Scalable object detection using deep neural networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2014: 2147-2154.Ren S, He K, Girshick R, et al. Faster r-cnn: Towards real-time object detection with region proposal networks[C]//Advances in neural information processing systems. 2015: 91-99.He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 770-778. CNN用于图像分割Schulz H, Behnke S. Learning Object-Class Segmentation with Convolutional Neural Networks[C]//ESANN. 2012.Farabet C, Couprie C, Najman L, et al. Scene parsing with multiscale feature learning, purity trees, and optimal covers[J]. arXiv preprint arXiv:1202.2160, 2012.Farabet C, Couprie C, Najman L, et al. Learning hierarchical features for scene labeling[J]. IEEE transactions on pattern analysis and machine intelligence, 2013, 35(8): 1915-1929.Dai J, He K, Sun J. Convolutional feature masking for joint object and stuff segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3992-4000. Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3431-3440. 刘丹,刘学军,王美珍. 一种多尺度CNN的图像语义分割算法[J]. 遥感信息,2017,(01):57-64.蒋应锋,张桦,薛彦兵,周冕,徐光平,高赞. 一种新的多尺度深度学习图像语义理解方法研究[J]. 光电子·激光,2016,(02):224-230. Mostajabi M, Yadollahpour P, Shakhnarovich G. Feedforward semantic segmentation with zoom-out features[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2015: 3376-3385.Lin G, Shen C, van den Hengel A, et al. Efficient piecewise training of deep structured models for semantic segmentation[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 3194-3203.","tags":[{"name":"技巧","slug":"jq","permalink":"http://abumaster.com/tags/jq/"}]},{"title":"Caffe的C++接口调用","date":"2017-04-18T12:25:06.000Z","path":"2017/04/18/Caffe的C-接口调用/","text":"Caffe的原生接口是C++，但是使用起来相对于Python和MATLAB也是最麻烦的，一个是需要配置各种第三方库，二是Windows下用VS新建C++工程出现各种编译问题。 1.配置第三方库Windows上运行的是官方的 Caffe-Windows 项目，第三方库是从别人打包好的下载，主要分为几大类：boost、gflags、glog、hdf5、LevelDB、lmdb、OpenBLAS、OpenCV、protobuf。配置内容包括（调试器最好配置Release版本的x64平台）：头文件新建一个工程，打开项目-&gt;工程属性页，C/C++ -&gt; 常规 -&gt; 附加包含目录，添加caffe相关的头文件,caffe及第三方依赖库，我的如下：1D:\\caffeDev\\caffe-master\\include;D:\\caffeDev\\NugetPackages\\boost.1.59.0.0\\lib\\native\\include;D:\\caffeDev\\NugetPackages\\OpenCV.2.4.10\\build\\native\\include;D:\\caffeDev\\NugetPackages\\gflags.2.1.2.1\\build\\native\\include;D:\\caffeDev\\NugetPackages\\glog.0.3.3.0\\build\\native\\include;D:\\caffeDev\\NugetPackages\\hdf5-v120-complete.1.8.15.2\\lib\\native\\include;D:\\caffeDev\\NugetPackages\\lmdb-v120-clean.0.9.14.0\\lib\\native\\include;D:\\caffeDev\\NugetPackages\\protobuf-v120.2.6.1\\build\\native\\include;D:\\caffeDev\\NugetPackages\\OpenBLAS.0.2.14.1\\lib\\native\\include; 附加库目录链接器 -&gt; 常规 -&gt; 附加库目录 ，添加内容为lib库所在的目录：1&lt;AdditionalLibraryDirectories&gt;D:\\caffeDev\\NugetPackages\\glog.0.3.3.0\\build\\native\\lib\\x64\\v120\\Release\\dynamic;D:\\caffeDev\\caffe-master\\Build\\x64\\Release;D:\\caffeDev\\NugetPackages\\OpenCV.2.4.10\\build\\native\\lib\\x64\\v120\\Release;D:\\caffeDev\\NugetPackages\\boost_date_time-vc120.1.59.0.0\\lib\\native\\address-model-64\\lib;D:\\caffeDev\\NugetPackages\\boost_filesystem-vc120.1.59.0.0\\lib\\native\\address-model-64\\lib;D:\\caffeDev\\NugetPackages\\boost_system-vc120.1.59.0.0\\lib\\native\\address-model-64\\lib;D:\\caffeDev\\NugetPackages\\protobuf-v120.2.6.1\\build\\native\\lib\\x64\\v120\\Release;D:\\caffeDev\\NugetPackages\\boost_thread-vc120.1.59.0.0\\lib\\native\\address-model-64\\lib;D:\\caffeDev\\NugetPackages\\boost_chrono-vc120.1.59.0.0\\lib\\native\\address-model-64\\lib;D:\\caffeDev\\NugetPackages\\hdf5-v120-complete.1.8.15.2\\lib\\native\\lib\\x64;D:\\caffeDev\\NugetPackages\\gflags.2.1.2.1\\build\\native\\x64\\v120\\dynamic\\Lib;D:\\caffeDev\\NugetPackages\\OpenBLAS.0.2.14.1\\lib\\native\\lib\\x64;%(AdditionalLibraryDirectories)&lt;/AdditionalLibraryDirectories&gt; 依赖项输入 -&gt; 附加依赖项，中填写需要的链接库，为上述目录中的链接库：12345678910111213141516171819202122232425262728293031323334353637383940&lt;AdditionalDependencies&gt;opencv_calib3d2410.lib;opencv_contrib2410.lib;opencv_core2410.lib;opencv_features2d2410.lib;opencv_flann2410.lib;opencv_gpu2410.lib;opencv_highgui2410.lib;opencv_imgproc2410.lib;opencv_legacy2410.lib;opencv_ml2410.lib;opencv_nonfree2410.lib;opencv_objdetect2410.lib;opencv_ocl2410.lib;opencv_photo2410.lib;opencv_stitching2410.lib;opencv_superres2410.lib;opencv_ts2410.lib;opencv_video2410.lib;opencv_videostab2410.lib;libglog.lib;caffe.lib;libprotobuf.lib;libcaffe.lib;gflags.lib;gflags_nothreads.lib;hdf5.lib;hdf5_cpp.lib;hdf5_f90cstub.lib;hdf5_fortran.lib;hdf5_hl.lib;hdf5_hl_cpp.lib;hdf5_hl_f90cstub.lib;hdf5_hl_fortran.lib;hdf5_tools.lib;szip.lib;zlib.lib;libopenblas.dll.a;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; 2.运行时问题实际新建一个项目（从caffe工程中拷的源码，配置好一切环境），可以编译成功，但是运行时出现问题：F0519 14:54:12.494139 14504 layer_factory.hpp:77] Check failed: registry.count(type) == 1 (0 vs. 1) Unknown layer type: Input (known types: Input )一者说，只有在caffe解决方案中新建项目，才可以正常运行问题。还有利用别人改进的caffe来减少外部的依赖关系，dtmoodie。另外一种，解决方法主要解决方案是将caffe中的各层都放进一个头文件中包含进工程中，1234#include \"caffe/common.hpp\" #include \"caffe/layers/input_layer.hpp\"extern INSTANTIATE_CLASS(InputLayer);//添加层信息REGISTER_LAYER_CLASS(Input);//注册层信息 可以完美运行了。","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]},{"title":"高效分片训练结构化模型用于图像语义分割","date":"2017-04-17T08:58:15.000Z","path":"2017/04/17/高效分片训练结构化模型用于图像语义分割/","text":"cvpr 论文：Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation 利用上下文信息提高分割图的精度。从两个方面入手：物体与物体之间、物体与背景之间。前者使用CNN结合CRF，来构造临近像素之间的关系；后者通过多尺度图像输入和滑动的金字塔池化。 特点： 制定了基于CNN的在CRFs上总体分段潜在函数模型用于衡量语义图像片之间的关系； 分段训练CRFs，避免重复推导，提高速度； 多尺度图像输入，用于探索图像背景和前景上下文信息； Featmap-Net是一个卷积网络，用于输出特征图，低分辨率的特征图； 创建CRF图，首先对于卷积网络生成的特征图，创建一些边界框，在边界框内被认为空间近似，顶点才会全连接，不同空间会创建不同的边界框。 上下文深度CRFs分为一元组的图的顶点和二元组的图的边，分别对应一个能量函数，通过一元和二元网络对应生成了类别的预测。利用背景上下文产生特征图的网络： 首先将输入图像缩放为三个不同的大小，放入网络，共享权重。图像缩放大小为1.2,0.8,0.4，再经过一层独立的卷积产生多尺度特征图，然后经过滑动金字塔池化产生了组合的特征图，金字塔池化如下图所示：使用双线性上采样和简单的边界优化对粗糙的预测结果进行后期处理，可能又更复杂的优化方式，比如：训练反卷积网络，训练复杂的从粗糙到精细的网络，利用中间特征图到高分辨率的预测。","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"},{"name":"计算机视觉","slug":"computerversion","permalink":"http://abumaster.com/tags/computerversion/"}]},{"title":"拉普拉斯重建和细化用于图像语义分割","date":"2017-04-16T08:05:56.000Z","path":"2017/04/16/拉普拉斯重建和细化用于图像语义分割/","text":"来自论文：Laplacian Reconstruction and Refinement for Semantic Segmentation，论文有两个贡献：1）证明了卷积特征图的空间低分辨率，但是在高维特征表示中包含重要的子像素定位信息；2）提出了一种类似拉普拉斯金字塔的多分辨率重建结构，对高分辨率特征图的跳跃连接可以从低分辨率特征图重建并成功细化分割图像的边界。 空间语义不确定性原则，探索在CNN特征层次结构中的空间和语义正确性。网络的顶层图像语义预测准确，但是带来的缺陷是在低分辨率下的图像空间上的定位，边界清晰但是标签有噪声。提出了一种重建模型在给定层次上提高空间定位的准确性，和一种细化技术用来融合多层的信息来优化图像的语义分割结果。与传统FCN的区别：不同之处在于，上采样和重建。CNN 特征图固有的缺少空间细节信息用一些不同的方法可以解决，如条件随机场、超像素、边界检测。还有一些成对的特征映射，可以进行反向传播进行训练。此论文的方法是直接提高输出激活图空间分辨率。双线性上采样是从低分辨率特征图中计算出高分辨率分割图的一种标准方法，首先卷积网络从特征图中计算出低分辨率的得分图，然后使用线性过滤器上采样为高分辨率的得分图。这种方法可能会从多通道低分辨率特征图中丢失定位信息。为了保留更多的空间信息，论文避免了将高维特征图折叠成低分辨率的类别预测。取而代之的是利用高分辨率基函数的线性组合对高分辨率的分类得分图的空间模式进行编码，这些函数的权重被高维特征图预测得到。实现：将高分辨率的特征图分成不重叠的图像块，大小取决于网络中池化层的个数次幂，通过一个卷积网络预测从高纬度低分辨率到特征图像块的映射。这些特征图像块和类别系数与一个基本函数集合相乘，再与一个基本的去卷积层相加，得到期望的全分辨率类别图。连接样条插值更高阶的样条插值替代上采样。学习基本函数基本结构首先，网络从上到下，分辨率越来越小。在每一次缩小时，特征图重建，再进行组合，产生对应倍数特征图的分割得分图，上图的水平方向，通过组合不同倍数的得分图。一种从高分辨率中减去低频成分的方法，边界masking，孤立出边界成分。金字塔的应用：下层分割图得分上采样作为上一层采样的参考，用于得分图和像素对的产生。Conclusion 以特定类重建为基础作为上采样； 合成低分辨率的语义丰富的特征图和拥有更多空间特性的高分辨率特征图，多层拉普拉斯金字塔重建结构。 最后可以加上CRF进行后期处理，优化结果。","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"},{"name":"计算机视觉","slug":"computerversion","permalink":"http://abumaster.com/tags/computerversion/"}]},{"title":"网易2017春招笔试编程题","date":"2017-04-14T06:14:18.000Z","path":"2017/04/14/网易2017春招笔试编程题/","text":"感悟：读的算法书，练习的算法题目都学到狗身上去了？不能活学活用，也就不能灵光乍现，难以进步。看似简单的题目，往往没有经过深思熟虑，导致复杂度高，无法通过。欠缺思考。","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"},{"name":"算法","slug":"algorithm","permalink":"http://abumaster.com/tags/algorithm/"}]},{"title":"FCN图像语义分割计算的细节问题","date":"2017-04-11T08:59:57.000Z","path":"2017/04/11/FCN图像语义分割计算的细节问题/","text":"Fully Convolutional Networks for Semantic Segmentation论文中的源代码阅读笔记。详细描述了分类网络如何变为一个分割网络，并输出最后的分割图。 从论文中地址中，下载FCN源码到本地。 1.使用现有模型进行图像语义分割 解压源代码，在根目录下，有一个infer.py的文件，打开，配置自己的模型路径，运行即可。 1234567891011121314151617181920212223import numpy as npfrom PIL import Imageimport matplotlib.pyplot as pltimport caffe# load image, switch to BGR, subtract mean, and make dims C x H x W for Caffeim = Image.open('voc-fcn8s/21.jpg')in_ = np.array(im, dtype=np.float32)in_ = in_[:,:,::-1]#in_ -= np.array((104.00698793,116.66876762,122.67891434))in_ -= np.array((106.08069,103.75618,100.05657))in_ = in_.transpose((2,0,1))# load netnet = caffe.Net('voc-fcn8s/deploy.prototxt', 'voc-fcn8s/fcn8s-heavy-pascal.caffemodel', caffe.TEST)# shape for input (data blob is N x C x H x W), set datanet.blobs['data'].reshape(1, *in_.shape)net.blobs['data'].data[...] = in_# run net and take argmax for predictionnet.forward()out = net.blobs['score'].data[0].argmax(axis=0)#print outplt.imshow(out,cmap='gray');plt.axis('off')plt.savefig('test1.png') 2.源码阅读 在源码的voc-fcn32s问价夹下，net.py用于生成网络的配置文件：train.prototxt、val.prototxt，solve.py用来运行训练网络，solver.prototxt是训练的配置文件：12345678910111213141516171819train_net: &quot;train.prototxt&quot;test_net: &quot;val.prototxt&quot;test_iter: 736# make test net, but don&apos;t invoke it from the solver itselftest_interval: 999999999display: 20average_loss: 20lr_policy: &quot;fixed&quot;# lr for unnormalized softmaxbase_lr: 1e-10# high momentummomentum: 0.99# no gradient accumulationiter_size: 1max_iter: 100000weight_decay: 0.0005snapshot: 4000snapshot_prefix: &quot;snapshot/train&quot;test_initialization: false solve.py 文件解读它调用了根目录下的 surgery.py 和 score.py 文件，后面再介绍。主要作用： 用现有的分类网络模型初始化网络； 自定义上采样层的卷积核； 加载验证图片，自定义最后的得分输出。初始化网络： 12345678weights = '../ilsvrc-nets/vgg16-fcn.caffemodel' #加载训练好的分类模型solver = caffe.SGDSolver('solver.prototxt')#加载网络配置文件solver.net.copy_from(weights)#从模型中复制权重#也可以写为如下方式：#solver = caffe.SGDSolver('solver.prototxt')#vgg_net = caffe.Net('solver.prototxt', weights, caffe.TRAIN)#surgery.transplant(solver.net, vgg_net)#del vgg_net 调用surgery.py中的上采样层，双线性插值，将图像变为原始大小。 12interp_layers = [k for k in solver.net.params.keys() if 'up' in k]surgery.interp(solver.net, interp_layers) 得分层 12345# scoringval = np.loadtxt('../data/segvalid11.txt', dtype=str)#加载验证图片for _ in range(25): solver.step(4000) score.seg_tests(solver, False, val, layer='score')#测试网络的得分情况 surgery.py 文件解读主要作用是制作适用于给定长宽的双线性插值内核，用于上采样。主要函数为:123456789101112def upsample_filt(size): \"\"\" Make a 2D bilinear kernel suitable for upsampling of the given (h, w) size. \"\"\" factor = (size + 1) // 2 if size % 2 == 1: center = factor - 1 else: center = factor - 0.5 og = np.ogrid[:size, :size] return (1 - abs(og[0] - center) / factor) * \\ (1 - abs(og[1] - center) / factor) score.py 文件解读主要作用：计算当前网络分割图的准确性。主要有以下几个标准：mean loss, overall accuracy, per-class accuracy, per-class IU。如何计算的呢？首先理解两个函数：1234567891011121314151617181920212223242526272829#计算a和b对应相同的就在矩阵中对应坐标加1。a和b保存着各个像素的分的类别0-20共21类def fast_hist(a, b, n): k = (a &gt;= 0) &amp; (a &lt; n)#过滤掉多余的分类 #bincount用于统计在范围内出现的个数，即直方图，如果不够n^2个， #那就填充到n^2，这样可以reshpe为n*n的矩阵，正好表示分割图和正确标记图在相同 #类别上像素出现的个数 return np.bincount(n * a[k].astype(int) + b[k], minlength=n**2).reshape(n, n)#调用计算直方图函数，指定了数据来源def compute_hist(net, save_dir, dataset, layer='score', gt='label'): n_cl = net.blobs[layer].channels#得到score层的通道数，fcn中为21通道，21类物体 if save_dir:#是否将分割图保存为文件 os.mkdir(save_dir) #hist表示：分割图中21类和标记图21类出现的像素数 #如：在i,j像素位置上分割图标记为2类物体，而实际标记为3那么在hist（2,3）+=1 # 在i,j+1像素位置分割图标记2类物体，实际标记图也为2类，则hist(2,2)+=1 # 可以看出hist对角矩阵是正确的分割； hist = np.zeros((n_cl, n_cl))#初始化21*21的二维矩阵 loss = 0 for idx in dataset: net.forward()#网络向前传播 #展开为一维数组 hist += fast_hist(net.blobs[gt].data[0, 0].flatten(), net.blobs[layer].data[0].argmax(0).flatten(),n_cl) if save_dir: im = Image.fromarray(net.blobs[layer].data[0].argmax(0).astype(np.uint8), mode='P') im.save(os.path.join(save_dir, idx + '.png')) # compute the loss as well 计算网络的损失 loss += net.blobs['loss'].data.flat[0]#flat[0]取第一个数 return hist, loss / len(dataset) 计算几个分割效果指标：1234567891011121314#mean loss print '&gt;&gt;&gt;', datetime.now(), 'Iteration', iter, 'loss', loss # overall accuracy acc = np.diag(hist).sum() / hist.sum()#对角线正确像素/总像素 print '&gt;&gt;&gt;', datetime.now(), 'Iteration', iter, 'overall accuracy', acc # per-class accuracy acc = np.diag(hist) / hist.sum(1)#每一类的 print '&gt;&gt;&gt;', datetime.now(), 'Iteration', iter, 'mean accuracy', np.nanmean(acc) # per-class IU iu = np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist)) print '&gt;&gt;&gt;', datetime.now(), 'Iteration', iter, 'mean IU', np.nanmean(iu) freq = hist.sum(1) / hist.sum() print '&gt;&gt;&gt;', datetime.now(), 'Iteration', iter, 'fwavacc', \\ (freq[freq &gt; 0] * iu[freq &gt; 0]).sum() 其他文件：训练文件的输入层类型是Python，作者自定义了一个voc_layers.py的Python数据加载层。 setup函数，设置voc训练集的路径，及中值文件，挑选数据的随机数； load_image和load_label函数，用于从数据集中加载图像和标记图像，并转换成数组形式，图像减去中值并转换成chanl*height*weight形式，label变为1*height*weight形式； forward和backward函数，前向传播将图像、标签复制到top[0]和top[1]中，反向传播不需要任何操作。 学习到的东西Python中numpy中的一些函数，诸如bincount、flatten、diag等。","tags":[{"name":"学习","slug":"学习","permalink":"http://abumaster.com/tags/学习/"},{"name":"caffe","slug":"caffe","permalink":"http://abumaster.com/tags/caffe/"}]},{"title":"Stanford Background Dataset介绍和使用","date":"2017-04-10T06:35:07.000Z","path":"2017/04/10/Stanford-Background-Dataset介绍和使用/","text":"Stanford Background Dataset是一个从各个数据库（LabelMe, MSRC, PASCAL VOC, Geometric Context）中精选出715张室外图像分为sky, tree, road, grass, water, building, mountain, foreground object共八大类的图像。 images文件夹包含了715张图像； horizons.txt 图像名称、大小、水平线位置； labels/*.regions.txt 标识每个像素的语义，0-7代表八类语义； labels/*.surfaces.txt 标识每个像素的几何类别（天空，水平，垂直）； labels/*.layers.txt 表示不同图像区域的整数矩阵。 读取图像和分割图像1.首先读取标签文件123456789101112131415vector&lt;char&gt; vec;//保存像素标记void readlabel(string labelname)&#123; ifstream infile(labelname.c_str(), std::ios::in); char line[1024] = &#123; 0 &#125;; while (infile.getline(line, sizeof(line))) &#123; stringstream word(line); char ch; while (word &gt;&gt; ch) &#123; vec.push_back(ch); &#125; &#125;&#125; 2.显示分割图像，根据语义标签，设置不同的颜色以区别 //显示分割图像 Mat colorim(im.rows, im.cols, CV_8UC3); int index = 0; //遍历所有像素，并设置像素值 for (int i = 0; i &lt; colorim.rows; ++i) { //获取第 i 行首像素指针 Vec3b * p = colorim.ptr&lt;Vec3b&gt;(i); for (int j = 0; j &lt; colorim.cols; ++j) { int lab = vec[index++]; switch (lab) { case '0'://sky p[j][0] = 128; //Blue p[j][1] = 128; //Green p[j][2] = 128; //Red break; case '1'://tree p[j][0] = 84; //Blue p[j][1] = 230; //Green p[j][2] = 80; //Red break; case '2'://road p[j][0] = 115; //Blue p[j][1] = 0; //Green p[j][2] = 100; //Red break; case '3'://grass p[j][0] = 0; //Blue p[j][1] = 255; //Green p[j][2] = 0; //Red break; case '4'://water p[j][0] = 255; //Blue p[j][1] = 0; //Green p[j][2] = 0; //Red break; case '5'://building p[j][0] = 0; //Blue p[j][1] = 0; //Green p[j][2] = 160; //Red break; case '6'://mountain p[j][0] = 63; //Blue p[j][1] = 214; //Green p[j][2] = 8; //Red break; case '7'://obj p[j][0] = 37; //Blue p[j][1] = 159; //Green p[j][2] = 230; //Red break; default://somthing else p[j][0] = 255; //Blue p[j][1] = 255; //Green p[j][2] = 255; //Red break; } } } imshow(\"分割图\", colorim); 结果","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"dataset","slug":"dataset","permalink":"http://abumaster.com/tags/dataset/"}]},{"title":"caffe提取各层特征","date":"2017-04-09T12:07:29.000Z","path":"2017/04/09/caffe提取各层特征/","text":"根据薛开宇的caffe学习笔记中逐层可视化特征进行实践，中间出现了一些问题，记录如下：1.caffe创建分类器Classifier继承自Net，可以从网络配置文件和模型中初始化网络，提供了一个predict函数，输入是一幅图片(w*H*K)返回的是一张N*C的numpy.ndarry。代表每张图片有可能对应的C个类别。也就是说，你以后用这个class会很方便，直接省去图片的初始化。源码位于：caffe-master\\python\\caffe下。 初始化这个分类器的时候，出现了一个问题：12345678net = caffe.Classifier(caffe_root + 'models/bvlc_reference_caffenet/deploy.prototxt',caffe_root + 'models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel')net.set_phase_test()net.set_mode_cpu()net.set_mean('data', caffe_root + 'python/caffe/imagenet/ilsvrc_2012_mean.npy')net.set_channel_swap('data', (2,1,0)) net.set_input_scale('data', 255) 就是在网络设置时，一直提示没有set_phase_test(*)的成员函数，试了几个平台都是如此提示，后来在网上找到了一点信息其中提到了，可以直接创建的时候初始化，对应于函数的声明所需的参数：12345net = caffe.Classifier(MODEL_FILE, PRETRAINED, mean=np.load(os.path.join(CAFFE_DIR, 'python/caffe/imagenet/ilsvrc_2012_mean.npy')), channel_swap=(2, 1, 0), raw_scale=255, image_dims=(256, 256)) 维度不匹配问题代码如下：1234567 File &quot;one.py&quot;, line 14, in &lt;module&gt; image_dims=(256, 256)) File &quot;/home/zgf/caffe-master/python/caffe/classifier.py&quot;, line 34, in __init__ self.transformer.set_mean(in_, mean) File &quot;/home/zgf/caffe-master/python/caffe/io.py&quot;, line 259, in set_mean raise ValueError(&apos;Mean shape incompatible with input shape.&apos;)ValueError: Mean shape incompatible with input shape. 中值文件读取的错误，在网络上找到了解决方案，将读取中值文件改为：mean=np.load(&#39;/home/zgf/caffe-master/python/caffe/imagenet/ilsvrc_2012_mean.npy&#39;).mean(1).mean(1)可以解决。 2.显示特征图像问题按照文档描述依次往下进行，文档使用的工具为ipython，显示图片用：ipt.show()，而我用的工具是jupyter，所以一直找不到这个命令，无法查看图像，从网上查到，可以在代码前面加上一句%matplotlib inline然后用import matplotlib as plt plt.imshow(img)实现。 3.结果加载网络123456789101112131415161718192021222324import numpy as npimport matplotlib.pyplot as pltcaffe_root='/home/zgf/caffe-master/'import sysimport ossys.path.insert(0, caffe_root + 'python/caffe')import caffeplt.rcParams['figure.figsize'] = (10, 10)plt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'ref_model_file = caffe_root+'/models/bvlc_reference_caffenet/deploy.prototxt'ref_pretrained = caffe_root+'/models/bvlc_reference_caffenet/bvlc_reference_caffenet.caffemodel'net = caffe.Classifier(ref_model_file, ref_pretrained, mean=np.load('/home/zgf/caffe-master/python/caffe/imagenet/ilsvrc_2012_mean.npy').mean(1).mean(1), channel_swap=(2,1,0), raw_scale=255, image_dims=(256, 256))scores = net.predict([caffe.io.load_image(caffe_root + 'examples/images/cat.jpg')])#显示网络的结构信息[(k, v.data.shape) for k, v in net.blobs.items()] [out]： 123456789101112131415[(&apos;data&apos;, (10, 3, 227, 227)), (&apos;conv1&apos;, (10, 96, 55, 55)), (&apos;pool1&apos;, (10, 96, 27, 27)), (&apos;norm1&apos;, (10, 96, 27, 27)), (&apos;conv2&apos;, (10, 256, 27, 27)), (&apos;pool2&apos;, (10, 256, 13, 13)), (&apos;norm2&apos;, (10, 256, 13, 13)), (&apos;conv3&apos;, (10, 384, 13, 13)), (&apos;conv4&apos;, (10, 384, 13, 13)), (&apos;conv5&apos;, (10, 256, 13, 13)), (&apos;pool5&apos;, (10, 256, 6, 6)), (&apos;fc6&apos;, (10, 4096)), (&apos;fc7&apos;, (10, 4096)), (&apos;fc8&apos;, (10, 1000)), (&apos;prob&apos;, (10, 1000))] 显示参数信息1[(k, v[0].data.shape) for k, v in net.params.items()] [out]：12345678[(&apos;conv1&apos;, (96, 3, 11, 11)), (&apos;conv2&apos;, (256, 48, 5, 5)), (&apos;conv3&apos;, (384, 256, 3, 3)), (&apos;conv4&apos;, (384, 192, 3, 3)), (&apos;conv5&apos;, (256, 192, 3, 3)), (&apos;fc6&apos;, (4096, 9216)), (&apos;fc7&apos;, (4096, 4096)), (&apos;fc8&apos;, (1000, 4096))] 输入层12345678910111213141516171819202122232425%matplotlib inlinedef showimage(im): if im.ndim == 3: m = im[:, :, ::-1] plt.imshow(im)def vis_square(data, padsize=1, padval=0): data -= data.min() data /= data.max() # force the number of filters to be square n = int(np.ceil(np.sqrt(data.shape[0]))) padding = ((0, n ** 2 - data.shape[0]), (0, padsize), (0, padsize)) + ((0, 0),) * (data.ndim - 3) data = np.pad(data, padding, mode='constant', constant_values=(padval, padval)) # 对图像使用滤波器 data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3) + tuple(range(4, data.ndim + 1))) data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:]) showimage(data) #plt.imshow(data)# index four is the center crop# 输出输入的图像image = net.blobs['data'].data[4].copy()image -= image.min()image /= image.max()showimage(image.transpose(1, 2, 0))#plt.imshow(image.transpose(1,2,0)) [out]：第一个卷积层，参数有[weight, biases]对应索引0,1。的96个过滤器：12345filters = net.params['conv1'][0].datavis_square(filters.transpose(0, 2, 3, 1))#96 feature mapfeat = net.blobs['conv1'].data[4, :96]vis_square(feat, padval=1) [out]：第二卷积层的过滤器，每个尺寸5*5*48，显示前48个，机器对应的输出只显示36张。12345filters = net.params['conv2'][0].datavis_square(filters[:48].reshape(48**2, 5, 5))feat = net.blobs['conv2'].data[4, :36]vis_square(feat, padval=1) [out]：接下来的卷积层的提取和输出一样：1234567891011feat = net.blobs['conv3'].data[4]vis_square(feat, padval=0.5)feat = net.blobs['conv4'].data[4]vis_square(feat, padval=0.5)#第5卷积层feat = net.blobs['conv5'].data[4]vis_square(feat, padval=0.2)#池化层feat = net.blobs['pool5'].data[4]vis_square(feat, padval=1) 最后的全连接层fc6和fc7，输出直方图： 1234567891011feat = net.blobs['fc6'].data[4]plt.subplot(2, 1, 1)plt.plot(feat.flat)plt.subplot(2, 1, 2)_ = plt.hist(feat.flat[feat.flat &gt; 0], bins=100)feat = net.blobs['fc7'].data[4]plt.subplot(2, 1, 1)plt.plot(feat.flat)plt.subplot(2, 1, 2)_ = plt.hist(feat.flat[feat.flat &gt; 0], bins=100) 最后的输出层，显示1000类概率的直方图信息： 123feat = net.blobs['prob'].data[4]plt.subplot(2, 1, 1)plt.plot(feat.flat) 显示最后的类别信息top5： 12345678imagenet_labels_filename = caffe_root + 'data/ilsvrc12/synset_words.txt'try: labels = np.loadtxt(imagenet_labels_filename, str, delimiter='\\t')except: !../data/ilsvrc12/get_ilsvrc_aux.shlabels = np.loadtxt(imagenet_labels_filename, str, delimiter='\\t')top_k = net.blobs['prob'].data[4].flatten().argsort()[-1:-6:-1]print labels[top_k] [out]：12345[&apos;n02123045 tabby, tabby cat&apos; &apos;n02123159 tiger cat&apos; &apos;n02124075 Egyptian cat&apos; &apos;n02119022 red fox, Vulpes vulpes&apos; &apos;n02127052 lynx, catamount&apos;]","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"caffe","slug":"caffe","permalink":"http://abumaster.com/tags/caffe/"}]},{"title":"结合特定任务边缘检测的图像语义分割","date":"2017-04-07T07:38:22.000Z","path":"2017/04/07/结合特定任务边缘检测的图像语义分割/","text":"DeepLab作者团队另一篇论文：Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform[1]。指出传统的FCN-CRF模型最后基于图模型的全连接条件随机场虽然可以定位物体边界更加准确，但是它的计算代价大，因而提出了一种新的解决方案，空间转换（DT）替换crfs，这是一种边缘过滤保留方法。计算速度有一定的提升。 主要思想取代最后的全连接条件随机场和与其关联的双向过滤器，变为域变换（DT）一种边缘感知过滤器。域变换的递归公式等于信号的自适应递归滤波，其中信息不允许在某些参考信号中跨越边缘传播。速度快。前期工作 图像语义分割网络中最大池化和下采样的出现，使稠密网络最后的输出图无法精准定位物体的边界信息，为了解决这个问题，出现了很多解决方案：组合中间特征图信息；反卷积和上采样；超像素等底层的分割方法；条件随机场，利用像素之间的依赖关系。 边缘检测学习物体的边界直接优化图像语义分割的表现。 长距离依赖（Long range dependency）通过DT输入进行反向传播，以共同学习端对端可训练系统中的分割图得分和边缘图。提出模型论文中提出的模型图：分为三个部分：1.语义分割预测，得出一个大致的分割图，与全卷积网络输出图类似；2.边缘预测网络，生成一个边缘预测图；3.域转换，使用物体边界限制分割图。 x表示需要过滤的原始信号量，y表示域转换密度信号d。使用递归公式计算，初始化y1=x1，然后递归计算i=2,...,N：$$y_i=(1-w_i)x_i+w_iy_{i-1}$$其中权重wi的计算依赖di：$$w_i=exp(-\\sqrt2d_i/{\\sigma_{s}})$$一维计算树，前向和反向传播的计算：$$\\frac{\\partial L}{\\partial x_i}\\leftarrow (1-w_i)\\frac{\\partial L}{\\partial y_i}$$$$\\frac{\\partial L}{\\partial w_i}\\leftarrow \\frac{\\partial L}{\\partial w_i}+(y_{i-1}-x_i)\\frac{\\partial L}{\\partial y_i}$$$$\\frac{\\partial L}{\\partial y_{i-1}}\\leftarrow \\frac{\\partial L}{\\partial y_{i-1}}+w_i\\frac{\\partial L}{\\partial y_i}$$源码和模型地址。接下来学习。 参考文献[1] “Semantic Image Segmentation with Task-Specific Edge Detection Using CNNs and a Discriminatively Trained Domain Transform” Liang-Chieh Chen, Jonathan T. Barron, George Papandreou, Kevin Murphy, and Alan L. Yuille In Conference on Computer Vision and Pattern Recognition (CVPR), 2016","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"}]},{"title":"进制转换","date":"2017-04-06T13:16:07.000Z","path":"2017/04/06/进制转换/","text":"题目描述将任意长度的二进制转换成十进制。要求任意长度，所以，不能常规的按照整型或长整型来表示，任意长度的数字，这里还要考虑溢出。因此，考虑字符串表示，同样的题目还有：数字的n次方、大数相加、大数相乘。主要思想：用字符串或者数组保存数字，计算时利用进位和标准运算进行。 解法二进制转换成十进制观察：10001000的计算过程，转换成十进制为：2^7+0+0+0+2^3+0+0+0。因此问题分为两个部分：计算二进制位置上为1时对应的十进制数是多少；对所有的位置得到的数字求和。代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * bin2dec 二进制转换成十进制 * @param decnum 十进制数字串 * @param n 二进制1后面的0的个数 */void bin2dec(int *decnum, int n)&#123; int index = LEN-1; decnum[index] = 1; int jinwei = 0; while (n--) //总共几个0 &#123; for(int i = LEN-1; i&gt;=0; i--) &#123; int nowtemp = 2*decnum[i]+jinwei; if(nowtemp&gt;=10)//需要进位 &#123; decnum[i] = nowtemp%10; //改变当前的数值 jinwei = nowtemp/10; //进位的多少 &#125; else &#123; decnum[i] = nowtemp; jinwei=0; &#125; &#125; &#125;&#125;/** * 将两个大数合并，放入左边数组 * @param left 相加结果放入此 * @param right 数组 */void sumbignum(int *left, int *right, int n=LEN)&#123; int jinwei = 0; for(int i=n-1; i&gt;=0; i--) &#123; int temp = left[i]+right[i]+jinwei;//俩数之和加上进位标志 if(temp &gt;= 10)//需要进位的 &#123; left[i] = temp%10; jinwei = temp/10; &#125; else //不用进位 &#123; left[i] = temp; jinwei = 0; &#125; &#125;&#125; 十进制转换成二进制同理，位数少的十进制转换成二进制一般应用除2求余，然后直到商为0。参考。对于大数，可以保存在一个数组中，用前一位的余数与当前的位数拼成一个数，除以2，商替换原数字对应的位数上，余数更新，直到把数字的位数计算完，算作得出二进制的一位（最后得出的余数）。直到商为0结束。代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include &lt;iostream&gt;#include &lt;string.h&gt;using namespace std;const int LEN = 100;/** * 检查数组代表的数字是否为空 * @param arr [description] * @param len [description] * @return [description] */bool IsZero(int *arr, int len)&#123; bool ret = true; for(int i=0; i&lt;len; i++) &#123; if(arr[i] != 0) &#123; ret = false; break; &#125; &#125; return ret;&#125;/** * 十进制转换成二进制的核心函数 * @param decnum 十进制保存位置 * @param binnum 二进制字符串 * @param len 十进制长度 */void dec2binCore(int *decnum, int *binnum, int len)&#123; int mod; int index = LEN-1; while(!IsZero(decnum, len))//十进制表示的数字不为0 &#123; mod = 0; for (int i =0; i&lt;len; i++) &#123; int tempnum = 10*mod+decnum[i]; int sang = tempnum/2; mod = tempnum%2; decnum[i] = sang; //更新商 &#125; binnum[index--] = mod;//最后的余数是二进制 &#125;&#125;void PrintInt(int *arr, int n)&#123; int start = 0;//bug for (int i=0; i&lt;n-1; i++) &#123; int j = i+1; if(arr[i]==0 &amp;&amp; arr[j]!=0 &amp;&amp; !start) &#123; start = 1; &#125; if (start) cout &lt;&lt; arr[j]; &#125; cout &lt;&lt; endl;&#125;void testdec2bin()&#123; string strnum; while(cin &gt;&gt; strnum) &#123; int len = strnum.size(); int *decnum = new int[len]; memset(decnum, 0, len*sizeof(int)); for(int i=0; i&lt;len; i++) decnum[i]= strnum[i]-'0'; int *binnum = new int[LEN]; memset(binnum, 0, LEN*sizeof(int)); dec2binCore(decnum, binnum, len); PrintInt(binnum, LEN); &#125;&#125;int main()&#123; testdec2bin(); system(\"pause\"); return 0;&#125; 感悟 看似简单的问题，还要细思量。","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"},{"name":"技巧","slug":"jq","permalink":"http://abumaster.com/tags/jq/"}]},{"title":"全卷积网络和全连接条件随机场","date":"2017-04-05T07:08:55.000Z","path":"2017/04/05/全卷积网络和全连接条件随机场/","text":"来自论文Semantic image segmentation with deep convolutional nets and fully connected crfs 2015. 主要针对将深度卷积网络应用到图像标记任务中的两个问题：下采样，和从分类网络中获得以物体为中心的描述。提出了像素级别的条件随机场和基于DCNN的一元项的结合的模型。优点：速度快，正确度高，模型简单。 主要创新点：1.带孔卷积在最后两个池化层后，跳过子采样，修改之后的卷积过滤器，变为卷积层。命名为孔算法，解释如图： 高效的特征提取算法，有效的稠密滑动窗口特征提取器 控制接受域大小，加速卷积网络的计算 2.边界恢复问题目前定位物体边界的挑战主要从两个方面： 利用融合不同层特征图的相关信息，估计物体边界 利用超像素表征，将任务委托给低层次的分割任务 模型： $$E(x)=\\sum_i\\theta_i(x_i)+\\sum_{ij}\\theta_{ij}(x_i,x_j)$$有一元项和二元项。3.多尺度预测为了增加边界定位的准确性，用了多尺度预测。具体是，为输入图像和第一个四层最大池化层附加一个双层MLP（第一层：128 个3*3的卷积核，第二层：128个1*1的卷积核）与最后一层的特征图连接。汇总的特征图，放入softmax层，产生5*128=640通道。 系统实现DeepLab：使用深度卷积网络，atrous卷积和全连接crfs的图像语义分割模型。针对传统方法的不足： 减少特征解析度（重复的最大池化和下采样） 存在多个尺度的对象 由于深度网络的稳定性导致定位精度下降提出的优化方案： 不采样 atrous spatial pyramid pooling 空间金字塔池化 结合条件随机场细节atrous卷积的计算，一维信号量示例如图：$$y[i]=\\sum_{k=1}^Kx[i+r\\cdot{k}]w[k]$$全连接条件随机场：关于能量函数，第一项由预测网络给出的预测值；第二项：公式分为两项，第一项是节点值不相等时为1，相等时为0，为了表示不同的标签将要受到惩罚。第二项，有两个高斯核组成，第一个是用像素的位置和像素的值表示，第二个是用像素之间的位置表示，他们是不同空间的特征。","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"}]},{"title":"Linux配置OpenCV","date":"2017-04-03T08:23:00.000Z","path":"2017/04/03/Linux配置OpenCV/","text":"源码安装OpenCV从OpenCV官网下载，最新版的OpenCV（opencv-3.2.0）。解压文件，得到文件夹（opencv-3.2.0），并进入；进行源码编译： 12345mkdir release cd release cmake -D CMAKE_BUILD_TYPE=RELEASE -D CMAKE_INSTALL_PREFIX=/usr/local .. make sudo make install 配置依赖库安装完成后，编译完成，运行时会出现找不到依赖库的情况如：error while loading shared libraries: libopencv_core.so.2.4: cannot open shared object file: No such file or directory这是因为没有把共享库放在加载器可以找到的位置，解决方法：首先定位到Opencv动态库所在的目录，一般在/usr/local/lib/或者/usr/lib/x86_64-linux-gun/中，在/etc/ld.so.conf.d/目录下创建一个opencv.conf的文件，并把上述的路径写入文件，然后执行sudo ldconfig -v编译1.第一种方式g++ DisplayImage.cpp -o DisplayImage &#39;pkg-config opencv --cflags --libs&#39;在上面的编译命令中我们其实用到了一个工具“pkg-config”，它主要有以下几个功能： 检查库的版本号。如果所需要的库的版本不满足要求，它会打印出错误信息，避免链接错误版本的库文件。 获得编译预处理参数，如宏定义，头文件的位置。 获得链接参数，如库及依赖的其它库的位置，文件名及其它一些连接参数。 自动加入所依赖的其它库的设置 2.cmake工具CMake工具，需要一个CMakeLists.txt文件，然后输入命令cmake .会生成Makefile文件，然后make就行了。CMakeLists.txt文件书写（opencv源码中带的例子example_cmake文件夹）：1234567891011121314151617181920212223242526# CMakeLists.txt# 必须的信息cmake_minimum_required(VERSION 2.8)# 工程的名称project(opencv_example_project)# 查找opencv的包find_package(OpenCV REQUIRED)# 打印库的状态信息message(STATUS &quot;OpenCV library status:&quot;)message(STATUS &quot; version: $&#123;OpenCV_VERSION&#125;&quot;)message(STATUS &quot; libraries: $&#123;OpenCV_LIBS&#125;&quot;)message(STATUS &quot; include path: $&#123;OpenCV_INCLUDE_DIRS&#125;&quot;)if(CMAKE_VERSION VERSION_LESS &quot;2.8.11&quot;) # Add OpenCV headers location to your include paths include_directories($&#123;OpenCV_INCLUDE_DIRS&#125;)endif()# 生成的目标以及源文件add_executable(opencv_example example.cpp)# 程序与opencv动态库连接target_link_libraries(opencv_example $&#123;OpenCV_LIBS&#125;)","tags":[{"name":"技巧","slug":"jq","permalink":"http://abumaster.com/tags/jq/"}]},{"title":"C++类构造函数","date":"2017-04-02T06:45:54.000Z","path":"2017/04/02/C-类构造函数/","text":"C++中构造函数和析构函数应该注意的问题构造方法用来初始化类的对象，与父类的其它成员不同，它不能被子类继承（子类可以继承父类所有的成员变量和成员方法，但不继承父类的构造方法）。因此，在创建子类对象时，为了初始化从父类继承来的数据成员，系统需要调用其父类的构造方法。C++11新标准中，派生类可以重用其直接基类定义的构造函数，类不能继承默认、拷贝、移动构造函数，如果派生类没有指定，则编译器会自动合成。 构造原则如下： 如果子类没有定义构造方法，则调用父类的无参数的构造方法。 如果子类定义了构造方法，不论是无参数还是带参数，在创建子类的对象的时候,首先执行父类无参数的构造方法，然后执行自己的构造方法。 在创建子类对象时候，如果子类的构造函数没有显示调用父类的构造函数，则会调用父类的默认无参构造函数。 在创建子类对象时候，如果子类的构造函数没有显示调用父类的构造函数且父类自己提供了无参构造函数，则会调用父类自己的无参构造函数。 在创建子类对象时候，如果子类的构造函数没有显示调用父类的构造函数且父类只定义了自己的有参构造函数，则会出错（如果父类只有有参数的构造方法，则子类必须显示调用此带参构造方法）。 如果子类调用父类带参数的构造方法，需要用初始化父类成员对象的方式 析构函数基类的析构函数声明为虚函数，这样销毁对象时子类会调用子类的析构函数，防止内存泄漏。如果没有定义为虚析构函数，销毁一个子类或者父类对象时，都会调用父类析构函数。","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]},{"title":"数据结构-红黑二叉树","date":"2017-04-01T04:25:53.000Z","path":"2017/04/01/数据结构-红黑二叉树/","text":"红黑树（Red Black Tree）是一种自平衡二叉查找树，是在计算机科学中用到的一种数据结构，典型的用途是实现关联数组。它是在1972年由Rudolf Bayer发明的，当时被称为平衡二叉B树（symmetric binary B-trees）。后来，在1978年被 Leo J. Guibas 和 Robert Sedgewick 修改为如今的“红黑树”。红黑树和AVL树（平衡二叉树）类似，都是在进行插入和删除操作时通过特定操作保持二叉查找树的平衡，从而获得较高的查找性能，而统计性能要优于AVL树，广泛应用到各种程序库中。 它虽然是复杂的，但它的最坏情况运行时间也是非常良好的，并且在实践中是高效的： 它可以在O(log n)时间内做查找，插入和删除，这里的n是树中元素的数目。性质红黑树是每个节点都带有黑色或者红色的二叉查找树。具有二叉树的性质，并且具有以下几个性质： 根节点是黑色 叶子节点（空节点）是黑色的 每个红色节点的两个子节点都是黑色的，叶子到根的路径上不能有连续的红色节点 从任一节点开始到其每个叶子节点的所有路径包含相同数目的黑色节点基本操作左旋、右旋、重新着色三个操作。右旋操作类似，左旋就是将旋转的节点变为左子树，提取右节点上来，右旋是将右旋节点变为右子树，提取左节点上来。 插入","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]},{"title":"动态规划-背包问题","date":"2017-03-31T00:41:50.000Z","path":"2017/03/31/动态规划-背包问题/","text":"题目描述有 a，b，c 三个物体，重量记为 W 5，4，3价值记为 V 20 10 12有一个背包容量 C = 10 ，问：可以装的最大价值为多少？ 解决动态规划问题的主要方法是找到状态转移方程，动态规划全局最优包含了局部最优解。分析上述问题：背包容量10，首先，第一个物品有装入和不装入两种情况，转入的话状态变为：容量5，物品重量4,3物品价值10,12；不装入则变为：容量10，物品质量4,3，价值10,12。因此可以定义：dp[i][j]表示前i个物品装到剩余容量为j的背包中的价值 dp[3][10]即为所求的结果，有了状态，这个状态是如何转移的呢？由上面的分析，可知，第i个物品有装入和不装入两种情况，因此状态转移方程可以表示如下：dp[i][j] = Max(dp[i-1][j], dp[i-1][j-w[i]]+v[i])。容易写出代码：123456789for(int i=0; i&lt;n; i++)&#123; for(int j=0; j&lt;=C; j++) &#123; dp[i][j] = (i==0?0:dp[i-1][j]); if(i&gt;0 &amp;&amp; j&gt;=W[i]) dp[i][j] = Max(dp[i-1][j], dp[i-1][j-w[i]]+v[i]); &#125;&#125; 关于优化空间复杂度上述存储状态方程为二维数组，可以压缩为一维数组，dp[i][j]变为dp[j]避免了重复的计算。123456789memeset(dp, 0, sizeof(dp));for(int i=0; i&lt;n; i++)&#123; for(int j=C; j&gt;=0; j++) &#123; if(i&gt;0 &amp;&amp; j&gt;=W[i]) dp[j] = Max(dp[j], dp[j-W[i]]+V[i]); &#125;&#125; [网易2017实习笔试题-双核处理]题目描述：一种双核CPU的两个核能够同时的处理任务，现在有n个已知数据量的任务需要交给CPU处理，假设已知CPU的每个核1秒可以处理1kb，每个核同时只能处理一项任务。n个任务可以按照任意顺序放入CPU进行处理，现在需要设计一个方案让CPU处理完这批任务所需的时间最少，求这个最小的时间。输入描述： 输入包括两行： 第一行为整数n(1 ≤ n ≤ 50) 第二行为n个整数lengthi，表示每个任务的长度为length[i]kb，每个数均为1024的倍数。 输出描述： 输出一个整数，表示最少需要处理的时间 输入输出例子： 53072 3072 7168 3072 10249216 解题思路:双核可以同时运行，故可以把任务分成两组，交由两个核顺序执行，最短执行时间取决于最后一个执行完成的时间，因此，两个数组长度相差越小，执行的时间也是越短的，换句话说，使其中一个数组无限接近输入数据总长度的一半sum/2即可。执行的时间为sum-sum/2。可以变为简单的背包问题：背包容量sum/2，物体重量为输入数据的长度，尽可能装满背包。状态转移方程可以记为：dp[i][j] = max(dp[i-1][j], dp[i-1][j-w[i]]+w[i]), dp[i][j]表示前i个物品在体积为j时可以填充的重量。 同样可以压缩数组变为一维，如上。 代码：123456789101112131415161718192021#include &lt;iostream&gt; using namespace std; int dp[210000]; int n,arr[51]; int main() &#123; int n; scanf(\"%d\",&amp;n); int sum = 0; for(int i = 0 ; i &lt; n ; i ++)&#123; scanf(\"%d\",&amp;arr[i]); arr[i] /= 1024; sum += arr[i]; &#125; memset(dp, 0, sizeof(dp)); for(int i = 0 ; i &lt; n ; i ++) for(int j = sum/2 ; j &gt;= arr[i] ; --j) dp[j] = max(dp[j],dp[j-arr[i]]+arr[i]); printf(\"%d\\n\",(sum-dp[sum/2])*1024); return 0; &#125;","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]},{"title":"深度解析网络用于图像语义分割","date":"2017-03-30T02:20:52.000Z","path":"2017/03/30/深度解析网络用于图像语义分割/","text":"2015 年 ICCV 论文：Semantic Image Segmentation via Deep Parsing Network [1]，针对图像语义分割将丰富信息（上下文关系）并入马尔科夫随机场（MRF），取代用迭代法去优化 MRFs 提出了一种卷积网络，被称为深度解析网络（DPN），可以通过一次前向传递决定端对端的计算。 $$E(y)=\\sum_{\\forall{i}\\in \\upsilon}\\Phi({y_i^u})+\\sum_{\\forall{i,j}\\in\\varepsilon}\\Psi(y_i^u,y_j^v)$$ 主要贡献：使用DPN交叉训练VGG16网络，通过一次迭代近似MF，减少计算量并且保证性能。MRF对于一副图片，看成一个无向图。边代表像素之间的联系，顶点是一个二值隐变量可以看成像素i是否分到标签u。如公式所示：能量函数可以写为：\\y ,\\upsilon ,\\varepsilon\\分别代表了潜变量、顶点和边。上述能量函数分为一元项和二元项，很明显，一元项是一个预测值，表示预测像素是某一个标签，二元项则是代表一组平滑约束。$$\\Phi{(y_i^u)}=-\\ln{p\\left(y_i^u=1|I\\right)}$$像素i用标签u表示的可能性。对于二元项是距离和共存性的乘积决定的。不可能共存，则值很大。如果两个像素临近并且相似，那么将会被鼓励分配相同的标签。这种衡量方法有两个主要的缺点：1.第一项是从训练数据中获得两个标签同时发生的频率来衡量，忽略了两个标签的空间上下文信息，比如人可以出现在桌子旁边，但是不太可能在桌子下面或者上面。空间上下文是一个混合模式，不同物体的形态可能出现在不同的图片中。2.只在像素间定义了成对的关系，没有考虑到高阶的相互作用。不理解，先放着。","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"}]},{"title":"公式专辑","date":"2017-03-29T07:53:21.000Z","path":"2017/03/29/公式专辑/","text":"公式的使用，论文中经常用到公式，在本地编写文章时常有 word 自带的基本上可以解决问题，当用 Markdown 书写时，又不想贴图，只好用在线的公式编辑器，一般有两种方法，一是在线生成公式，并引出外链，直接嵌入到文章中；另外一种用Mathjax引擎，引入一个脚本，在文章中编辑。 1.MathJax 引擎参考stackexchange，很简单引入一个脚本：1&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt; 然后编写公式一个栗子：1$$\\sum_&#123;i=0&#125;^n i^2 = \\frac&#123;(n^2+n)(2n+1)&#125;&#123;6&#125;$$ 就生成了：$$\\sum_{i=0}^n i^2 = \\frac{(n^2+n)(2n+1)}{6}$$注意：想要在浏览器上预览，需要更改markdown priview的配置文件，运行mathjax运行。 2.在线 LaTeX 公式编辑器在线LaTeX公式编辑器，使用LaTeX公式。在其中编写好公式后，直接生成了一段html代码，直接复制到 Markdown 文本中。如代码:1&lt;a href=&quot;https://www.codecogs.com/eqnedit.php?latex=$$f(x)=\\sum_&#123;i=1&#125;^n&amp;space;a_i$$&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;https://latex.codecogs.com/gif.latex?$$f(x)=\\sum_&#123;i=1&#125;^n&amp;space;a_i$$&quot; title=&quot;$$f(x)=\\sum_&#123;i=1&#125;^n a_i$$&quot; /&gt;&lt;/a&gt; 生成：优点：可以本地预览；缺点：只是引用的图片，右键不可操作，公式大的话可能加载慢，图片不清晰。 3.基本语法 无论使用哪种方式，公式的基本语法是相同的。常用的总结如下。 公式样式行内公式\\\\(公式\\\\)，行间公式$$公式$$**空格的表示：`\\quad’表示一个quad空格 字符\\为转义符，特殊字符前要加。 上下标用^表示上标，用_表示下标 字母上下标记用\\overline{}表示上划线，用\\underline{}表示下划线；用\\hat{}表示字母上面有一个小尖角，而\\widehat{}表示有一个大尖角; 用\\bar{} \\acute{} \\check{} \\grave{}分别表示四个声调：一声平，二声扬，三声拐弯，四声降；\\tilde{}波浪线, \\vec{}向量,\\dot{}点。 希腊字符\\alpha, \\beta, …, \\omega: α,β,…ω；\\Gamma, \\Delta, …, \\Omega: Γ,Δ,…,Ω。 数学函数例如sin x要表示成\\sin x；log x要表示成\\log x；lim x表示成\\lim_{x\\to0}。 分数开方\\frac{ }{ }分数；\\sqrt{n}{r}表示开n次方。 括号和分割符() [] |是不变的； {}要转义，写为\\{\\}用\\left 和 \\right调整大小。 数学公式求和：\\sum_{i=0}^n{a_i}积分：\\int例子参见stackexchange","tags":[{"name":"技巧","slug":"jq","permalink":"http://abumaster.com/tags/jq/"}]},{"title":"图床测试","date":"2017-03-29T06:20:44.000Z","path":"2017/03/29/图床测试/","text":"图床一般是专门用来存储图片的服务器，同时向外提供链接。国内和国外之分，本次测试的是极简图床。优点：不必把图片上传到博客服务器，节省服务器的空间。缺点：不能上传大于5M的图片，稳定性待测。 来一波从wallhaven下载的图片","tags":[{"name":"技巧","slug":"jq","permalink":"http://abumaster.com/tags/jq/"}]},{"title":"Feedforward semantic segmentation with zoom-out features","date":"2017-03-27T06:21:29.000Z","path":"2017/03/27/Feedforward-semantic-segmentation-with-zoom-out-features/","text":"使用缩小特征的前馈语义分割 Feedforward semantic segmentation with zoom-out features 2015年CVPR论文，在PASCAL VOC 2012测试集上达到了69.9%的正确率。将小的图像元素（超像素）映射到丰富的特征表示中，这些特征是从嵌套的增加区域中获得。这些区域通过从超像素一直缩小到场景级别的分辨率获得。这种方法充分利用了图像和隐藏空间中的统计结构，而不显式设置结构化预测机制，从而避免了复杂、昂贵的推论。从而超像素是由多层前馈网络进行分类。 从大量的现代分割著作中，得到了一种被广泛接受的知识，分割可以看成一个结构化预测的任务，可以用条件随机场和结构化支持向量机模型。作者脱离传统，提出图像语义分割看作单阶段的分类任务，其中每个像素元素（超像素）被标记为一个标签，使用一个前馈模型，依据从图像计算的证据。用在前馈分类中的证据不是从孤立的局部区域中获得，而是从序列中获得，序列是怎么组成的呢？首先得到一个超像素，再向外扩展，获得一个更大的闭合区域，直到扩展到整张图片。计算每一个层次的丰富特征，结合所有特征，放入分类网络中。 ###缩小的特征融合将图像的类别分割转换成对一组超像素分类。由于我们期望为每个超级像素应用相同的分类机，我们希望超像素的性质是相似的，特别是它们的大小。使用了SLIC。本地超像素本身有很窄的范围，我们希望特征提取器可以捕获更多的本地信息：颜色，上下文，其他一些属性，在临近的超像素之间这些属性有很大的不同。近似距离场景","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"}]},{"title":"全卷积网络用于图像语义分割","date":"2017-03-25T07:31:02.000Z","path":"2017/03/25/全卷积网络用于图像语义分割/","text":"全卷积网络用于图像语义分割 (Fully Convolutional Networks for Semantic Segmentation)[1] 全卷积网络实际上就是把普通卷积网络的最后的全连接层变为卷积层，因为全连接层会把空间信息隐藏，全部展开为一维向量，换为卷积可以保留空间信息。如VGG-16网络在处理ImageNet数据集时，最后的1000个输出，是1000维向量，来表示1000类事物的概率，当换为卷积层时，输出了1000个1*1大小的输出，对此上采样，可以输出对应的heat-map。这是分类网络作为稠密输出的关键。 文章解决的问题是如何生成稠密的预测即dense prediction Shift-and-stitch假设原图和FCN输出图之间的降采样因子f，对于原图的每个f*f区域，对于0 &lt;= x,y &lt;f处理这 f2 个输入，并且交替输出，使得预测在接受域的中心像素。每个像素对应一个中心像素，因此为稠密输出。缺点：感受野没变，但是原图被划分为了f*f大小的图像片作为输入图像，使得网络无法感受更加精细的信息。 稀疏过滤器调整下采样过程中的步长，变为1，可以保证下采样不会损失图像的大小。缺点：下采样的功能被减弱，同时保留了更多信息，接受域相对变小，可能损失全局信息，同样为卷积层带来了更多的运算。 上采样上采样（Upsampling）也称反卷积（Deconvolution），参数和卷积一样可以在训练中学习。运算也和卷积类似，为逆过程。设输入大小w0*h0，经过卷积后的大小为w1*h1，计算公式如下：卷积运算：w1 = (w0 + 2*pad - kernelsize)/stride + 1h1 = (h0 + 2*pad - kernelsize)/stride + 1反卷积运算：w0 = (w1 - 1)*stride + kernelsize - 2*padh0 = (h1 - 1)*stride + kernelsize - 2*pad经过上采样后的图像可能会比原图大，需要裁剪为原图像大小，caffe中的crop层，提供了很好的算法。 语义分割的框架结构文中提出的框架结构如图所示：作者发现32倍率的上采样导致输出图非常粗糙，因此想出了利用上层的一些特征来优化输出图像，就有了FCN-16s和FCN-8s的方案，其主要思想是利用上层的池化层的信息，减少上采样的倍率，保留了更多的特征。 具体的实践针对传统网络的全连接层变为卷积层，如VGG-16网络中第一个卷积层是25088*4096，将之解释为512*7*7*4096。产生端对端的训练模型。在论文提供的源码中，FCN-32s的配置文件，第一个卷积层为： layer { name: &quot;conv1_1&quot; type: &quot;Convolution&quot; bottom: &quot;data&quot; top: &quot;conv1_1&quot; param { lr_mult: 1 decay_mult: 1 } param { lr_mult: 2 decay_mult: 0 } convolution_param { num_output: 64 pad: 100 #填充100 kernel_size: 3 stride: 1 } } 填充100的原因为：在VGG-16网络中卷积的参数，kernersize=3，stride=1，pad=1，所以卷积层不会改变图像的大小，所以图像只有在池化层改变大小，且变为原大小的一半。为了方便将图像看为一维的，设原图像大小h，经过了5层池化后，图像缩小了32倍，变为h5 = h/32，紧接着全连接层，可以看成是卷积层，卷积参数为：kernelsize=7 pad=0 stride=1，根据卷积计算公式，经过卷积层fc6后的输出图像大小为h6 = (h5-7)/1 + 1 = (h-192)/32 因此，图像小于192的就无法往下计算了，所以要pad=100，解决了网络输入图像固定大小的弊端，全卷积网络可以输入任意大小的图像。 例子根据FCN-32s的配置文件如果输入图像大小为3*320*320经过了卷积conv1的输出为：64*518*518经过了池化pool1的输出为：64*259*259经过了卷积conv2的输出为：128*259*259经过了池化pool2的输出为：128*130*130经过了卷积conv3的输出为：256*130*130经过了池化pool3的输出为：256*65*65经过了卷积conv4的输出为：512*65*65进过了池化pool4的输出为：512*32*32 经过了卷积conv5的输出为：512*32*32经过了池化pool5的输出为：512*16*16经过了fc6的卷积后输出为：4096*10*10经过了fc7的卷积后输出为：4096*10*10经过score_fr的卷积输出：21*10*10上采样（反卷积）输出为：21*352*352score层裁剪后输出为：21*320*320","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"}]},{"title":"卷积网络应该注意的问题","date":"2017-03-24T06:28:18.000Z","path":"2017/03/24/卷积网络应该注意的问题/","text":"卷积神经网络简介，由于其出色的特征提取特性，使得在计算机视觉方面有了很好的应用，并取得了出色的成绩。卷积卷积操作是卷积网络中的核心操作，其主要目的是为了提取图像的显著特征，降低特征维数，进而来减少计算量。在 caffe 代码中的主要参数如下： 123456789101112131415161718192021222324252627layer &#123; name: &quot;conv1&quot; type: &quot;Convolution&quot; bottom: &quot;data&quot; #上层是数据层 top: &quot;conv1&quot; param &#123; #权重学习参数 lr_mult: 1 #权重学习率 需要乘以基础学习率base\\_lr decay_mult: 1 &#125; param &#123; #偏置学习参数 lr_mult: 2 decay_mult: 0 &#125; convolution_param &#123; #卷积参数 num_output: 96 #卷积操作后的输出特征图 kernel_size: 11 #卷积核大小 stride: 4 #步长 #可能也有pad为扩充边缘 weight_filler &#123; #权值初始化 type: &quot;gaussian&quot; #类型为weight-filter 或xavier算法等，默认constant，全部0 std: 0.01 &#125; bias_filler &#123; #偏置的初始化 type: &quot;constant&quot; value: 0 &#125; &#125;&#125; 输入：n*c0*w0*h0输出：n*c1*w1*h2c1对应num_output，输出对应的大小计算:w1 = (w0 + 2*pad - kernersize)/stride + 1h1 = (h0 + 2*pad - kernelsize)/stride + 1在 caffe 源码中的计算是将图像和卷积核通过 im2col 转换成矩阵，再对两矩阵内积。 池化池化也称下采样，为了减少运算和数据维度的一种方式，被分为： 最大池化（Max Pooling），取最大值； 均值池化（Mean Pooling），取均值； 高斯池化。caffe 中的配置代码： 1234567891011layer &#123; name: &quot;pool1&quot; type: &quot;Pooling&quot; bottom: &quot;norm1&quot; top: &quot;pool1&quot; pooling_param &#123; #池化参数 pool: MAX #池化类型 kernel_size: 3 #池化核大小 stride: 2 #步长，重叠 &#125;&#125; 池化的计算公式与卷积操作类似：输入：n*c0*w0*h0输出：n*c1*w1*h2c1对应num_output，输出对应的大小计算:w1 = (w0 + 2*pad - kernersize)/stride + 1h1 = (h0 + 2*pad - kernelsize)/stride + 1 LRN层LRN全称为Local Response Normalization，即局部响应归一化层，没什么用，有一些网络中加入了这一层，对局部区域进行归一化，配置信息和参数说明如下：123456789101112layer &#123; name: &quot;norm1&quot; type: &quot;LRN&quot; bottom: &quot;conv1&quot; top: &quot;norm1&quot; lrn_param &#123; #参数 local_size: 5 #（1）通道间归一化时表示求和的通道数； #（2）通道内归一化时表示求和区间的边长； alpha: 0.0001 #缩放因子 beta: 0.75 #指数项 &#125;&#125; 激活函数 激活函数需要具有以下特性： 非线性； 单调、连续可微分； 范围不饱和，避免梯度为0； 原点近似线性。常用的激活函数有：Sigmoid 函数、Tanh 函数、ReLU 函数等。如 AlexNet 中用到的ReLU激活函数：$$f(x)=max(0,x)$$这种激活函数的特点是：无梯度损耗，收敛速度快，网络稀疏性大，计算量小。缺点是，梯度大的话，导致权重更新以后变大，输出0，使得神经元不再更新。因此要注意学习率的设置。 全连接层全连接层又称内积层（Inner-Product），是将特征图像全部展开为一维向量。caffe 中的文档显示： Inputn * c_i * h_i * w_i Outputn * c_o * 1 * 1这里引用了dupuleng的例子。lenet 网络配置文件中的一段： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647layers &#123; name: &quot;conv2&quot; type: CONVOLUTION bottom: &quot;pool1&quot; top: &quot;conv2&quot; blobs_lr: 1 blobs_lr: 2 convolution_param &#123; num_output: 50 kernel_size: 5 stride: 1 weight_filler &#123; type: &quot;xavier&quot; &#125; bias_filler &#123; type: &quot;constant&quot; &#125; &#125;&#125;layers &#123; name: &quot;pool2&quot; type: POOLING bottom: &quot;conv2&quot; top: &quot;pool2&quot; pooling_param &#123; pool: MAX kernel_size: 2 stride: 2 &#125;&#125;layers &#123; name: &quot;ip1&quot; type: INNER_PRODUCT bottom: &quot;pool2&quot; top: &quot;ip1&quot; blobs_lr: 1 blobs_lr: 2 inner\\_product\\_param &#123; num_output: 500 weight_filler &#123; type: &quot;xavier&quot; &#125; bias_filler &#123; type: &quot;constant&quot; &#125; &#125;&#125; conv2 的输入图像是256*27*27经过了卷积操作，输出50*22*22同样作为了pool2的输入，进行池化，pool2的输出50*11*11，下一层全连接层，输出500*1*1的向量，是如何进行计算的呢？要把所有通道全部展开做卷积，首先要把pool2输出的特征图展开为一维向量，共需要500*50*11*11个参数，进行卷积，输出500*1*1的一维向量。","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"caffe","slug":"caffe","permalink":"http://abumaster.com/tags/caffe/"}]},{"title":"Conditional Random Fields as Recurrent Neural Networks","date":"2017-03-21T08:00:18.000Z","path":"2017/03/21/CRFs-as-RNN/","text":"2015 年 ICCV 会议文章 Conditional Random Fields as Recurrent Neural Networks[1] 的阅读笔记。 关键词图像语义分割CRF as RNN摘要像素级别的标注任务，例如图像语义分割在图像理解方面占据着重要的作用。最近的方法开始利用深度学习技术在图像识别任务上的能力来解决像素级别的标注任务。现在的核心问题是深度学习方法在描绘可视化物体具有限制性。为了解决这个问题，我们提出了一个新形式的卷积网络，它结合了卷积网络的优势和条件随机场的概率图模型。为此，我们制定了使用高斯对模型和中值近似的条件随机场作为循环神经网络。这个网路就是 CRF-RNN 被嵌入到 CNN 中，最为一个集 CNNs 和 CRFs 优点于一体的深度网络。更重要的是，我们的系统完全在 CNNs 中集成了 CRF 模型，让使用传统的反向传播算法训练端对端的系统成为了可能，不需要额外的后期处理物体的边界。MarkDown 中使用公式 加入脚本定义，现在用到的是 MathJax 引擎12&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt; 使用Tex公式 $$行间公式；\\\\行内公式，参考MathJax basic tutorial and quick reference 示例$$x=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}$$ 引言低层次计算机视觉问题，为图像中的像素分配标签。特征表示在个体像素分类中占有重要的作用。同样要考虑到图像的边界和特征、空间关系，以此来获得较为准确地分割结果。设计出一个强大的特征表示器是像素级别标记的关键挑战。传统的方法不再讨论，现在深度学习的方法利用大尺度的卷积网络，在高层次视觉上取得了非常大的成果。这激励着利用卷积网络去解决低层次的问题。主要利用卷积网络提取特征替代以前的手工标注特征。将用于高层视觉的分类网络转换成低层次视觉的任务依然存在着一些问题提出了几个问题： 传统的卷积网络有大接受域的卷积过滤器，会产生比较粗糙的输出图。最大池化层的出现，过滤掉一些特征，导致了输出的分割图不够精细。 缺少了平滑度约束，没有考虑到相似的像素，空间或者外形相似的约束，导致了输出图的边界不明确，或者出现杂散区域。尤其是马尔科夫随机场（MRFs）和它的变体条件随机场（CRFs）已经成为应用到计算机视觉领域中一个成功的模型。用于像素标记的CRFs推理主要的思想是将语义标签分配问题转换成概率推理问题，包括了相似像素之间一致性并入假设。CRFs可以微调分割图的细节，优化边界问题，克服了单纯利用CNNs的缺点。用CRFs作为后期的处理，无法发挥出CRF的优势，卷积网络在训练的阶段也无法根据CRF的表现来调整权重。本文将CNN与CRF结合为一个统一的框架，可以共同训练。相关工作许多方法用深度学习来解决图像语义分割问题，可以归为以下两个类别： 特征提取和分割分离开的策略。使用CNN提取有意义的图像特征，利用超像素去构造图像的模式。首先从图像中获得超像素，再用特征提取器提取特征。存在着一个致命的缺点，前期如果有误差，后面误差越来越大。与他们的方案不同，此文用典型的图模型CRF可以被作为RNN，指定为深度网络的一部分。结合CNN实现端对端的训练。 直接学习从原始图像到标记图像的非线性模型。例如FCN等网络，去掉了最后的全连接层变为卷积层。全连接条件随机场 条件随机场进行图像语义分割的能量函数：定义隐变量Xi为像素点i的分类标签，取值范围为分类语义标签L={l1,l2,l3,…,ln}；Yi为每个随机变量Xi的观测值，即是每个像素的颜色值。条件随机场的目标就是通过观测变量Yi，推理出潜变量Xi的标签。对于一张图像，可以看成图模型G=(V,E)，每个顶点对应了V={X1,X2,...,Xn}，对于边来说，全连接的条件随机场，顶点与所有的点都有连线。条件随机场的目标函数：能量函数有一元势函数和二元势函数，分别表示了当像素点i的观测值是yi时，该像素点属于标签xi的概率。可以直接从cnn中计算出。二元是函数是两个像素值相似或者相邻则两个像素属于同一类的概率很大。实现 参考文献[1] Zheng S, Jayasumana S, Romera-Paredes B, et al. Conditional random fields as recurrent neural networks[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 1529-1537.","tags":[{"name":"深度学习","slug":"deeplearn","permalink":"http://abumaster.com/tags/deeplearn/"},{"name":"论文","slug":"lunwen","permalink":"http://abumaster.com/tags/lunwen/"}]},{"title":"柔性数组","date":"2017-03-12T13:59:48.000Z","path":"2017/03/12/柔性数组/","text":"C/C++中的0长数组 定义：柔性数组（Flexible Array）也叫伸缩性数组、变长数组。 作用 ：放入结构体中，可以存放动态长度的字符串、数组等。 用法举例： 放在结构体的最后，长度为0的数组。长度为0不占用任何空间，数组名只是一个符号，代表了一个不可改变的地址。 1234struct package &#123; int len; char data[0];&#125;; 用途：根据变长数组的特性很容易构造出一些数据结构，缓冲区、数据包等。不会浪费多余的空间，用多少申请多少。 使用: 假设用上面的结构来发送1024字节大小的数据包，首先要构造一个数据包：1234char *pMsg = (char *)malloc(sizeof(package)+1024); package *pPack = (package*)pMsg;pPack-&gt;len = 1024;memcpy(pPack-&gt;data, source, 1024); 强制类型转换，将package类型的指针指向了申请的内存的开始，分为两个部分：前一部分表示字符串的长度，后一部分表示实际的内容。将整个数据包发出去，不会浪费一点额外的空间，在网络中传输节省了流量，提升了速度。","tags":[{"name":"C++","slug":"C","permalink":"http://abumaster.com/tags/C/"}]}]