<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>张国丰</title>
  <subtitle>张国丰的博客</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://abumaster.com/"/>
  <updated>2017-03-25T04:43:14.744Z</updated>
  <id>http://abumaster.com/</id>
  
  <author>
    <name>abumaster</name>
    <email>1902819397@qq.com</email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="http://abumaster.com/2017/03/24/%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C%E5%BA%94%E8%AF%A5%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://abumaster.com/2017/03/24/卷积网络应该注意的问题/</id>
    <published>2017-03-24T06:28:18.819Z</published>
    <updated>2017-03-25T04:43:14.744Z</updated>
    
    <content type="html"><![CDATA[<hr>
<p>title: 卷积神经网络种应该注意的问题<br>date: 2017-03-24 14:28:18<br>categories: 深度学习</p>
<h2 id="tags-深度学习-caffe"><a href="#tags-深度学习-caffe" class="headerlink" title="tags: [深度学习,caffe]"></a>tags: [深度学习,caffe]</h2><p>卷积神经网络简介，由于其出色的特征提取特性，使得在计算机视觉方面有了很好的应用，并取得了出色的成绩。</p>
<p>###1.卷积神经网络的几种基本操作<br><strong>卷积</strong><br>卷积操作是卷积网络中的核心操作，其主要目的是为了提取图像的显著特征，降低特征维数，进而来减少计算量。在 caffe 代码中的主要参数如下：<br><a id="more"></a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">layer &#123;</div><div class="line">  name: &quot;conv1&quot;</div><div class="line">  type: &quot;Convolution&quot;</div><div class="line">  bottom: &quot;data&quot; #上层是数据层</div><div class="line">  top: &quot;conv1&quot;</div><div class="line">  param &#123; #权重学习参数</div><div class="line">    lr_mult: 1 #权重学习率 需要乘以基础学习率base\_lr</div><div class="line">    decay_mult: 1</div><div class="line">  &#125;</div><div class="line">  param &#123; #偏置学习参数</div><div class="line">    lr_mult: 2</div><div class="line">    decay_mult: 0</div><div class="line">  &#125;</div><div class="line">  convolution_param &#123; #卷积参数</div><div class="line">    num_output: 96 #卷积操作后的输出特征图</div><div class="line">    kernel_size: 11 #卷积核大小</div><div class="line">    stride: 4 #步长 #可能也有pad为扩充边缘 </div><div class="line">    weight_filler &#123; #权值初始化</div><div class="line">      type: &quot;gaussian&quot; #类型为weight-filter 或xavier算法等，默认constant，全部0</div><div class="line">      std: 0.01</div><div class="line">    &#125;</div><div class="line">    bias_filler &#123; #偏置的初始化</div><div class="line">      type: &quot;constant&quot;</div><div class="line">      value: 0</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">&#125;</div><div class="line">```  </div><div class="line">输入：`n*c0*w0*h0`  </div><div class="line">输出：`n*c1*w1*h2`  </div><div class="line">c1对应num_output，输出对应的大小计算:  </div><div class="line">`w1 = (w0 + 2*pad - kernersize)/stride + 1`  </div><div class="line">`h1 = (h0 + 2*pad - kernelsize)/stride + 1`  </div><div class="line">在 **caffe** 源码中的计算是将图像和卷积核通过 im2col 转换成矩阵，再对两矩阵内积。 </div><div class="line"></div><div class="line">**池化**  </div><div class="line">池化也称下采样，为了减少运算和数据维度的一种方式，被分为：  </div><div class="line">+	最大池化（Max Pooling），取最大值；  </div><div class="line">+	均值池化（Mean Pooling），取均值；  </div><div class="line">+	高斯池化。  </div><div class="line">**caffe** 中的配置代码：</div></pre></td></tr></table></figure></p>
<p>layer {<br>  name: “pool1”<br>  type: “Pooling”<br>  bottom: “norm1”<br>  top: “pool1”<br>  pooling_param { #池化参数<br>    pool: MAX #池化类型<br>    kernel_size: 3 #池化核大小<br>    stride: 2 #步长，重叠<br>  }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">池化的计算公式与卷积操作类似：  </div><div class="line">输入：`n*c0*w0*h0`  </div><div class="line">输出：`n*c1*w1*h2`  </div><div class="line">c1对应num_output，输出对应的大小计算:  </div><div class="line">`w1 = (w0 + 2*pad - kernersize)/stride + 1`  </div><div class="line">`h1 = (h0 + 2*pad - kernelsize)/stride + 1`  </div><div class="line"></div><div class="line">**LRN层**  </div><div class="line">LRN全称为Local Response Normalization，即局部响应归一化层，没什么用，有一些网络中加入了这一层，对局部区域进行归一化，配置信息和参数说明如下：</div></pre></td></tr></table></figure></p>
<p>layer {<br>  name: “norm1”<br>  type: “LRN”<br>  bottom: “conv1”<br>  top: “norm1”<br>  lrn_param { #参数<br>    local_size: 5 #（1）通道间归一化时表示求和的通道数；</p>
<pre><code>#（2）通道内归一化时表示求和区间的边长；
alpha: 0.0001 #缩放因子
beta: 0.75 #指数项
</code></pre><p>  }<br>}<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">**激活函数**</div><div class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;  </div><div class="line">激活函数需要具有以下特性：  </div><div class="line">+	非线性；  </div><div class="line">+	单调、连续可微分；  </div><div class="line">+	范围不饱和，避免梯度为0；  </div><div class="line">+	原点近似线性。  </div><div class="line">常用的激活函数有：**Sigmoid 函数**、**Tanh 函数**、**ReLU 函数**等。  </div><div class="line">如 **AlexNet** 中用到的ReLU激活函数：  </div><div class="line">$$f(x)=max(0,x)$$  </div><div class="line">这种激活函数的特点是：无梯度损耗，收敛速度快，网络稀疏性大，计算量小。缺点是，梯度大的话，导致权重更新以后变大，输出0，使得神经元不再更新。因此要注意学习率的设置。  </div><div class="line"></div><div class="line">**全连接层**  </div><div class="line">全连接层又称内积层（Inner-Product），是将特征图像全部展开为一维向量。  </div><div class="line">**caffe** 中的文档显示：  </div><div class="line">+  	Input  </div><div class="line">`n * c_i * h_i * w_i`  </div><div class="line">+	Output  </div><div class="line">`n * c_o * 1 * 1`  </div><div class="line">这里引用了[dupuleng](http://www.cnblogs.com/dupuleng/articles/4312149.html)的例子。  </div><div class="line">lenet 网络配置文件中的一段：</div></pre></td></tr></table></figure></p>
<p>layers {<br>  name: “conv2”<br>  type: CONVOLUTION<br>  bottom: “pool1”<br>  top: “conv2”<br>  blobs_lr: 1<br>  blobs_lr: 2<br>  convolution_param {<br>    num_output: 50<br>    kernel_size: 5<br>    stride: 1<br>    weight_filler {<br>      type: “xavier”<br>    }<br>    bias_filler {<br>      type: “constant”<br>    }<br>  }<br>}<br>layers {<br>  name: “pool2”<br>  type: POOLING<br>  bottom: “conv2”<br>  top: “pool2”<br>  pooling_param {<br>    pool: MAX<br>    kernel_size: 2<br>    stride: 2<br>  }<br>}<br>layers {<br>  name: “ip1”<br>  type: INNER_PRODUCT<br>  bottom: “pool2”<br>  top: “ip1”<br>  blobs_lr: 1<br>  blobs_lr: 2<br>  inner_product_param {<br>    num_output: 500<br>    weight_filler {<br>      type: “xavier”<br>    }<br>    bias_filler {<br>      type: “constant”<br>    }<br>  }<br>}<br><code>``  
conv2 的输入图像是</code>256<em>27</em>27<code>经过了卷积操作，输出</code>50<em>22</em>22<code>同样作为了pool2的输入，进行池化，pool2的输出</code>50<em>11</em>11<code>，下一层全连接层，输出</code>500<em>1</em>1<code>的向量，是如何进行计算的呢？要把所有通道全部展开做卷积，首先要把pool2输出的特征图展开为一维向量，共需要</code>500<em>50</em>11<em>11<code>个参数，进行卷积，输出</code>500</em>1*1`的一维向量。  </p>
]]></content>
    
    <summary type="html">
    
      &lt;hr&gt;
&lt;p&gt;title: 卷积神经网络种应该注意的问题&lt;br&gt;date: 2017-03-24 14:28:18&lt;br&gt;categories: 深度学习&lt;/p&gt;
&lt;h2 id=&quot;tags-深度学习-caffe&quot;&gt;&lt;a href=&quot;#tags-深度学习-caffe&quot; class=&quot;headerlink&quot; title=&quot;tags: [深度学习,caffe]&quot;&gt;&lt;/a&gt;tags: [深度学习,caffe]&lt;/h2&gt;&lt;p&gt;卷积神经网络简介，由于其出色的特征提取特性，使得在计算机视觉方面有了很好的应用，并取得了出色的成绩。&lt;/p&gt;
&lt;p&gt;###1.卷积神经网络的几种基本操作&lt;br&gt;&lt;strong&gt;卷积&lt;/strong&gt;&lt;br&gt;卷积操作是卷积网络中的核心操作，其主要目的是为了提取图像的显著特征，降低特征维数，进而来减少计算量。在 caffe 代码中的主要参数如下：&lt;br&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Conditional Random Fields as Recurrent Neural Networks</title>
    <link href="http://abumaster.com/2017/03/21/CRFs-as-RNN/"/>
    <id>http://abumaster.com/2017/03/21/CRFs-as-RNN/</id>
    <published>2017-03-21T08:00:18.000Z</published>
    <updated>2017-03-21T12:40:34.307Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>2015 年 ICCV 会议文章 Conditional Random Fields as Recurrent Neural Networks<sup>[1]</sup> 的阅读笔记。    </p>
</blockquote>
<p><strong>关键词</strong><br>图像语义分割<br>CRF as RNN<br><strong>摘要</strong><br>像素级别的标注任务，例如图像语义分割在图像理解方面占据着重要的作用。最近的方法开始利用深度学习技术在图像识别任务上的能力来解决像素级别的标注任务。现在的核心问题是深度学习方法在描绘可视化物体具有限制性。为了解决这个问题，我们提出了一个新形式的卷积网络，它结合了卷积网络的优势和条件随机场的概率图模型。为此，我们制定了使用高斯对模型和中值近似的条件随机场作为循环神经网络。这个网路就是 CRF-RNN 被嵌入到 CNN 中，最为一个集 CNNs 和 CRFs 优点于一体的深度网络。更重要的是，我们的系统完全在 CNNs 中集成了 CRF 模型，让使用传统的反向传播算法训练端对端的系统成为了可能，不需要额外的后期处理物体的边界。<br><a id="more"></a><br><del><strong>MarkDown 中使用公式</strong></del>  </p>
<ol>
<li>加入脚本定义，现在用到的是 MathJax 引擎<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;</div><div class="line">&lt;/script&gt;</div></pre></td></tr></table></figure>
</li>
</ol>
<p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>  </p>
<ol>
<li>使用Tex公式 $$行间公式；\\行内公式，参考<a href="https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference">MathJax basic tutorial and quick reference</a>  </li>
<li>示例<br>$$x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$$  </li>
</ol>
<hr>
<p><strong>引言</strong><br>低层次计算机视觉问题，为图像中的像素分配标签。特征表示在个体像素分类中占有重要的作用。同样要考虑到图像的边界和特征、空间关系，以此来获得较为准确地分割结果。<br>设计出一个<strong>强大的特征表示器是像素级别标记的关键挑战</strong>。传统的方法不再讨论，现在深度学习的方法利用大尺度的卷积网络，在高层次视觉上取得了非常大的成果。这激励着利用卷积网络去解决低层次的问题。主要利用卷积网络提取特征替代以前的手工标注特征。<br>将用于高层视觉的分类网络转换成低层次视觉的任务依然存在着一些问题提出了几个问题：  </p>
<ul>
<li>传统的卷积网络有大接受域的卷积过滤器，会产生比较粗糙的输出图。最大池化层的出现，过滤掉一些特征，导致了输出的分割图不够精细。  </li>
<li>缺少了平滑度约束，没有考虑到相似的像素，空间或者外形相似的约束，导致了输出图的边界不明确，或者出现杂散区域。<br>尤其是马尔科夫随机场（MRFs）和它的变体条件随机场（CRFs）已经成为应用到计算机视觉领域中一个成功的模型。<strong>用于像素标记的CRFs推理主要的思想是将语义标签分配问题转换成概率推理问题，包括了相似像素之间一致性并入假设</strong>。CRFs可以微调分割图的细节，优化边界问题，克服了单纯利用CNNs的缺点。<del>用CRFs作为后期的处理，无法发挥出CRF的优势，卷积网络在训练的阶段也无法根据CRF的表现来调整权重</del>。本文将CNN与CRF结合为一个统一的框架，可以共同训练。<br><strong>相关工作</strong><br>许多方法用深度学习来解决图像语义分割问题，可以归为以下两个类别：  </li>
</ul>
<ul>
<li>特征提取和分割分离开的策略。使用CNN提取有意义的图像特征，利用超像素去构造图像的模式。首先从图像中获得超像素，再用特征提取器提取特征。存在着一个致命的缺点，前期如果有误差，后面误差越来越大。与他们的方案不同，此文用典型的图模型CRF可以被作为RNN，指定为深度网络的一部分。结合CNN实现端对端的训练。  </li>
<li>直接学习从原始图像到标记图像的非线性模型。例如FCN等网络，去掉了最后的全连接层变为卷积层。<br><strong>全连接条件随机场</strong>  <div align=center> <img src="/photos/crfs.jpg" alt="CRFs"></div><br>条件随机场进行图像语义分割的能量函数：定义隐变量Xi为像素点i的分类标签，取值范围为分类语义标签L={l1,l2,l3,…,ln}；Yi为每个随机变量Xi的观测值，即是每个像素的颜色值。条件随机场的目标就是通过观测变量Yi，推理出潜变量Xi的标签。<br>对于一张图像，可以看成图模型<code>G=(V,E)</code>，每个顶点对应了<code>V={X1,X2,...,Xn}</code>，对于边来说，全连接的条件随机场，顶点与所有的点都有连线。<br>条件随机场的目标函数：<br>能量函数有一元势函数和二元势函数，分别表示了当像素点i的观测值是yi时，该像素点属于标签xi的概率。可以直接从cnn中计算出。二元是函数是两个像素值相似或者相邻则两个像素属于同一类的概率很大。<br><strong>实现</strong><br><div align=center> <img src="/photos/algo.png" alt="algorithm"></div>  


</li>
</ul>
<p><strong>参考文献</strong><br>[1] Zheng S, Jayasumana S, Romera-Paredes B, et al. Conditional random fields as recurrent neural networks[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 1529-1537.</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;2015 年 ICCV 会议文章 Conditional Random Fields as Recurrent Neural Networks&lt;sup&gt;[1]&lt;/sup&gt; 的阅读笔记。    &lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;关键词&lt;/strong&gt;&lt;br&gt;图像语义分割&lt;br&gt;CRF as RNN&lt;br&gt;&lt;strong&gt;摘要&lt;/strong&gt;&lt;br&gt;像素级别的标注任务，例如图像语义分割在图像理解方面占据着重要的作用。最近的方法开始利用深度学习技术在图像识别任务上的能力来解决像素级别的标注任务。现在的核心问题是深度学习方法在描绘可视化物体具有限制性。为了解决这个问题，我们提出了一个新形式的卷积网络，它结合了卷积网络的优势和条件随机场的概率图模型。为此，我们制定了使用高斯对模型和中值近似的条件随机场作为循环神经网络。这个网路就是 CRF-RNN 被嵌入到 CNN 中，最为一个集 CNNs 和 CRFs 优点于一体的深度网络。更重要的是，我们的系统完全在 CNNs 中集成了 CRF 模型，让使用传统的反向传播算法训练端对端的系统成为了可能，不需要额外的后期处理物体的边界。&lt;br&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://abumaster.com/categories/deeplearning/"/>
    
    
      <category term="论文" scheme="http://abumaster.com/tags/lunwen/"/>
    
      <category term="深度学习" scheme="http://abumaster.com/tags/deeplearn/"/>
    
  </entry>
  
  <entry>
    <title>柔性数组</title>
    <link href="http://abumaster.com/2017/03/12/%E6%9F%94%E6%80%A7%E6%95%B0%E7%BB%84/"/>
    <id>http://abumaster.com/2017/03/12/柔性数组/</id>
    <published>2017-03-12T13:59:48.000Z</published>
    <updated>2017-03-21T08:45:40.231Z</updated>
    
    <content type="html"><![CDATA[<h3 id="C-C-中的0长数组"><a href="#C-C-中的0长数组" class="headerlink" title="C/C++中的0长数组"></a>C/C++中的0长数组</h3><hr>
<p>定义：柔性数组（Flexible Array）也叫伸缩性数组、变长数组。<br></p>
<h2 id="作用-：放入结构体中，可以存放动态长度的字符串、数组等。"><a href="#作用-：放入结构体中，可以存放动态长度的字符串、数组等。" class="headerlink" title="作用 ：放入结构体中，可以存放动态长度的字符串、数组等。"></a>作用 ：放入结构体中，可以存放动态长度的字符串、数组等。</h2><ul>
<li>用法举例：<br><br>放在结构体的最后，长度为0的数组。长度为0不占用任何空间，数组名只是一个符号，代表了一个不可改变的地址。  </li>
</ul>
<a id="more"></a>
<figure class="highlight c"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">struct</span> package &#123;</div><div class="line">	<span class="keyword">int</span> len;</div><div class="line">	<span class="keyword">char</span> data[<span class="number">0</span>];</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<ul>
<li>用途：<br>根据变长数组的特性很容易构造出一些数据结构，缓冲区、数据包等。不会浪费多余的空间，用多少申请多少。<br></li>
<li>使用:<br>假设用上面的结构来发送1024字节大小的数据包，首先要构造一个数据包：<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">char *pMsg = (char *)malloc(sizeof(package)+1024); </div><div class="line">package *pPack = (package*)pMsg;</div><div class="line">pPack-&gt;len = 1024;</div><div class="line">memcpy(pPack-&gt;data, source, 1024);</div></pre></td></tr></table></figure>
</li>
</ul>
<p>强制类型转换，将package类型的指针指向了申请的内存的开始，分为两个部分：前一部分表示字符串的长度，后一部分表示实际的内容。将整个数据包发出去，不会浪费一点额外的空间，在网络中传输节省了流量，提升了速度。</p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;C-C-中的0长数组&quot;&gt;&lt;a href=&quot;#C-C-中的0长数组&quot; class=&quot;headerlink&quot; title=&quot;C/C++中的0长数组&quot;&gt;&lt;/a&gt;C/C++中的0长数组&lt;/h3&gt;&lt;hr&gt;
&lt;p&gt;定义：柔性数组（Flexible Array）也叫伸缩性数组、变长数组。&lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;作用-：放入结构体中，可以存放动态长度的字符串、数组等。&quot;&gt;&lt;a href=&quot;#作用-：放入结构体中，可以存放动态长度的字符串、数组等。&quot; class=&quot;headerlink&quot; title=&quot;作用 ：放入结构体中，可以存放动态长度的字符串、数组等。&quot;&gt;&lt;/a&gt;作用 ：放入结构体中，可以存放动态长度的字符串、数组等。&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;用法举例：&lt;br&gt;&lt;br&gt;放在结构体的最后，长度为0的数组。长度为0不占用任何空间，数组名只是一个符号，代表了一个不可改变的地址。  &lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
      <category term="学习" scheme="http://abumaster.com/categories/%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="C++" scheme="http://abumaster.com/tags/C/"/>
    
  </entry>
  
</feed>
